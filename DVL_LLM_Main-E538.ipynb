{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dtIq6xl8qZkd",
        "EgBnjqKhq351",
        "FNjjPm0mrDiW",
        "3-GWUD09rRJ-",
        "uwWZvJzhrdgo"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### required installations"
      ],
      "metadata": {
        "id": "GdKfeVy1uGRa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0W33STEoQQE",
        "outputId": "4efaf723-b8a1-4a1b-f39e-9f979b836485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
            "Requirement already satisfied: us in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: pyvis in /usr/local/lib/python3.11/dist-packages (0.3.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.49)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.22)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.17)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.11/dist-packages (from us) (1.1.0)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.11/dist-packages (from pyvis) (3.1.6)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pyvis) (4.0.5)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.11/dist-packages (from pyvis) (3.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.9.6->pyvis) (3.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (4.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.69.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain) (3.0.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.13)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain langchain-community langchain-experimental langchain-google-genai us pyvis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import required libraries"
      ],
      "metadata": {
        "id": "oP_kg3QqpQ1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "import pandas as pd\n",
        "from IPython.display import display, Markdown\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import random\n",
        "import getpass\n",
        "import os\n",
        "import io\n",
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "BhsPlFlXoVmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load the different dvl llm chains into memory"
      ],
      "metadata": {
        "id": "SX_pOGoGpVAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### User Insight Type"
      ],
      "metadata": {
        "id": "Y-ynwcwtpdWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def identify_insight(user_request, llm):\n",
        "    \"\"\"\n",
        "    Identifies the type of insight needed based on a user's visualization request.\n",
        "\n",
        "    Args:\n",
        "        user_request (str): The user's request for a visualization.\n",
        "        llm (LLM): The Language Model to process the request.\n",
        "\n",
        "    Returns:\n",
        "        dict: A structured dictionary containing:\n",
        "              - \"insight_need_type\": The classified insight type.\n",
        "              - \"key_variables\": A list of identified variables.\n",
        "              - \"reasoning\": Explanation of the classification.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define Prompt Template\n",
        "    identify_insight_prompt = PromptTemplate(\n",
        "        input_variables=[\"user_request\"],\n",
        "        template=\"\"\"\n",
        "        You are an expert in data visualization needs analysis. The user has provided the following visualization request: \"{user_request}\".\n",
        "\n",
        "        Below is a structured reference on different **Insight Need Types** that users may require when requesting data visualizations. Classify the user request into one of these **eight main categories** and determine if there is a **subtype**.\n",
        "\n",
        "        --- **Insight Need Types** ---\n",
        "\n",
        "        **1. Categorization and Clustering**\n",
        "           - Assigns data into distinct **categories or clusters**.\n",
        "           - Example: \"Group employees by department.\"\n",
        "\n",
        "        **2. Ordering, Ranking, and Sorting**\n",
        "           - Arranges objects based on ranking, value, or order.\n",
        "           - Example: \"Show the top 10 highest-paying jobs.\"\n",
        "\n",
        "        **3. Distribution (Outliers and Gaps)**\n",
        "           - Displays how objects are **spread across a dataset**, detecting **gaps or outliers**.\n",
        "           - Example: \"Show a histogram of employee salaries.\"\n",
        "\n",
        "        **4. Trends (Process and Time-Based Analysis)**\n",
        "           - Tracks **gradual changes over time** (short-term, long-term trends).\n",
        "           - Example: \"Plot revenue growth over the past five years.\"\n",
        "\n",
        "        **5. Comparison (Similarities and Differences)**\n",
        "           - Examines multiple objects to **highlight differences and similarities**.\n",
        "           - Example: \"Compare average salaries of men and women in the company.\"\n",
        "\n",
        "        **6. Geospatial Location (Mapping Data to Physical Spaces)**\n",
        "           - Assigns data to a **geographical location**.\n",
        "           - Example: \"Show a heatmap of customer purchases by region.\"\n",
        "\n",
        "        **7. Composition (Part-to-Whole Relationships and Text Structuring)**\n",
        "           - **Part-to-whole relationships** (e.g., pie charts, hierarchical structures).\n",
        "           - Example: \"Show the percentage breakdown of expenses by category.\"\n",
        "\n",
        "        **8. Correlations and Relationships (Finding Patterns Between Variables)**\n",
        "           - Identifies **connections between multiple variables** (one-to-one, many-to-many).\n",
        "           - Example: \"Analyze the relationship between years of experience and salary.\"\n",
        "\n",
        "        --- **Your Task** ---\n",
        "        - **Step 1:** Analyze the user request carefully.\n",
        "        - **Step 2:** Classify it into the **most relevant insight need type** from the list above.\n",
        "        - **Step 3:** If applicable, specify a **subtype** (e.g., \"Comparison → Side-by-side bar chart\").\n",
        "        - **Step 4:** Extract the **key variables** that should be used in the analysis (e.g., occupation, job_family).\n",
        "        - **Step 5:** Provide a brief **reasoning** explaining why the chosen insight need type and visualization subtype is appropriate.\n",
        "\n",
        "        --- **Expected Output Format** ---\n",
        "        Return the response in the following structured format:\n",
        "\n",
        "        **Insight Need Type**: [Selected Type] → [Subtype if applicable]\n",
        "        **Identified Variables**: [List of variables]\n",
        "        **Reasoning**: [Brief explanation on why this classification was chosen]\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Create the Chain\n",
        "    identify_insight_chain = LLMChain(\n",
        "        llm=llm,\n",
        "        prompt=identify_insight_prompt,\n",
        "        output_key=\"insight_analysis\"\n",
        "    )\n",
        "\n",
        "    # Display the user request\n",
        "    display(Markdown(\"### \"+ \"The user request is:\"))\n",
        "    display(Markdown(user_request))\n",
        "\n",
        "    # Run the Chain\n",
        "    result = identify_insight_chain.invoke({\"user_request\": user_request})\n",
        "\n",
        "    # Extract the raw output\n",
        "    insight_analysis = result[\"insight_analysis\"]\n",
        "\n",
        "    # Parse the structured output\n",
        "    def parse_insight_output(insight_text):\n",
        "        lines = insight_text.strip().split(\"\\n\")\n",
        "        lines = [line for line in lines if line != '']\n",
        "\n",
        "        # Ensure correct indexing\n",
        "        insight_need_type = lines[0].replace(\"**Insight Need Type**: \", \"\").strip()\n",
        "\n",
        "        # Extract key variables safely\n",
        "        key_variables_line = lines[1].replace(\"**Identified Variables**: \", \"\").strip()\n",
        "        key_variables = [var.strip() for var in key_variables_line.split(\",\") if var.strip()]\n",
        "\n",
        "        # Extract reasoning\n",
        "        reasoning = lines[2].replace(\"**Reasoning**: \", \"\").strip()\n",
        "\n",
        "        return {\n",
        "            \"insight_need_type\": insight_need_type,\n",
        "            \"key_variables\": key_variables,\n",
        "            \"reasoning\": reasoning\n",
        "        }\n",
        "\n",
        "    # Convert output into structured JSON\n",
        "    insight_json = parse_insight_output(insight_analysis)\n",
        "\n",
        "    # Display the final output\n",
        "    display(Markdown(\"### Identified Insight Needs: \"))\n",
        "    display(Markdown(\"Insight Need Type is \"+ insight_json[\"insight_need_type\"]))\n",
        "    display(Markdown(\"Key Variables to use : \" + str(insight_json[\"key_variables\"])))\n",
        "    display(Markdown(\"Reasoning : \" + insight_json[\"reasoning\"]))\n",
        "\n",
        "    return insight_json"
      ],
      "metadata": {
        "id": "u9cLSjriokJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### identify data scale"
      ],
      "metadata": {
        "id": "VW4R0OqNp7c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def identify_data_scale(df, insight_need_type, key_variables, llm):\n",
        "    \"\"\"\n",
        "    Identifies the data scale types of key variables in the dataset.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataset to analyze.\n",
        "        insight_need_type (str): The identified insight type.\n",
        "        key_variables (list): List of key variables relevant to the analysis.\n",
        "        llm (LLM): The Language Model to process the request.\n",
        "\n",
        "    Returns:\n",
        "        dict: A structured dictionary containing:\n",
        "              - \"missing_variables\": List of missing variables.\n",
        "              - \"categorized_variables\": Mapping of found variables to their data scale type and recommended Python data type.\n",
        "    \"\"\"\n",
        "\n",
        "    # Capture DataFrame info\n",
        "    df_info_str = df.dtypes.to_string()\n",
        "\n",
        "    # Convert df.head() and df.describe() to JSON for readability\n",
        "    df_head_str = df.head().to_json(orient=\"records\", indent=2)\n",
        "    df_summary_str = df.describe().to_json(indent=2)\n",
        "\n",
        "    # Define the Prompt Template\n",
        "    identify_data_scale_prompt = PromptTemplate(\n",
        "        input_variables=[\"df_info\", \"df_head\", \"df_summary\", \"insight_need_type\", \"key_variables\"],\n",
        "        template=\"\"\"\n",
        "        You are an expert in data analysis and need to classify dataset variables based on their **data scale types**.\n",
        "\n",
        "        --- **Data Scale Types** ---\n",
        "        1️⃣ **Nominal Scale (Categorical Data)**\n",
        "          - **Definition**: Qualitative scale with no inherent order.\n",
        "          - **Examples**: Gender (Male/Female), Job Titles, Countries.\n",
        "\n",
        "        2️⃣ **Ordinal Scale (Ranked Data)**\n",
        "          - **Definition**: Categories have a specific order, but differences are not measurable.\n",
        "          - **Examples**: Satisfaction Ratings (Strongly Agree → Agree → Neutral → Disagree).\n",
        "\n",
        "        3️⃣ **Interval Scale (Numeric Data Without Absolute Zero)**\n",
        "          - **Definition**: Numeric values with meaningful differences, but zero is arbitrary.\n",
        "          - **Examples**: Temperature in Celsius/Fahrenheit, IQ Scores.\n",
        "\n",
        "        4️⃣ **Ratio Scale (Numeric Data With Absolute Zero)**\n",
        "          - **Definition**: Numeric values where zero represents total absence.\n",
        "          - **Examples**: Population counts, Weight, Height, Income, Distance.\n",
        "\n",
        "        --- **Dataset Information** ---\n",
        "        **Insight Need Type**: {insight_need_type}\n",
        "        **Key Variables**: {key_variables}\n",
        "\n",
        "        --- **Dataset Overview** ---\n",
        "        **df.info() Output:**\n",
        "        {df_info}\n",
        "\n",
        "        **df.head() Output (Sample Rows):**\n",
        "        {df_head}\n",
        "\n",
        "        **df.describe() Output (Summary Statistics):**\n",
        "        {df_summary}\n",
        "\n",
        "        --- **Your Task** ---\n",
        "        - **Step 1:** Identify the actual dataset column names that match the **Key Variables**.\n",
        "          - If a key variable is missing, output: `\"missing_variables\": [list of missing variables]`.\n",
        "          - If a key variable exists in the dataset, return a **dictionary** mapping the **key variable** to the actual column name.\n",
        "          - Example:\n",
        "            ```json\n",
        "            {{\n",
        "                \"variable_mappings\": {{\n",
        "                    \"Income\": \"annual_salary\",\n",
        "                    \"Age\": \"customer_age\"\n",
        "                }}\n",
        "            }}\n",
        "            ```\n",
        "          - **IMPORTANT:** Use these mapped dataset column names in all further steps.\n",
        "\n",
        "        - **Step 2:** **Only classify the found variables (from Step 1) into one of the four Data Scales**:\n",
        "          - **Nominal Scale** → Categorical with no ordering.\n",
        "          - **Ordinal Scale** → Ordered categories without fixed differences.\n",
        "          - **Interval Scale** → Numeric with equal intervals, but no absolute zero.\n",
        "          - **Ratio Scale** → Numeric with equal intervals and absolute zero.\n",
        "\n",
        "        - **Step 3:** Determine the appropriate **Python data type** for each found variable.\n",
        "          - If the current type is incorrect, **generate a JSON output** specifying how to convert the variable.\n",
        "\n",
        "        --- **IMPORTANT: JSON OUTPUT ONLY** ---\n",
        "        Return ONLY valid **JSON**.\n",
        "        **DO NOT classify variables that are not listed in the \"Key Variables\".**\n",
        "        **DO NOT return Python code** or explanations.\n",
        "        **DO NOT enclose JSON in Python code blocks**.\n",
        "\n",
        "        --- **Expected Output Format (Strict JSON for Key Variables Only)** ---\n",
        "        ```json\n",
        "        {{\n",
        "            \"missing_variables\": [list of missing variables, if any],\n",
        "            \"categorized_variables\": {{\n",
        "                \"found_variable\": {{\n",
        "                    \"chosen_scale\": \"Nominal/Ordinal/Interval/Ratio\",\n",
        "                    \"suggested_python_dtype\": \"int/float/str/category\",\n",
        "                    \"conversion_needed\": true/false,\n",
        "                    \"conversion_code\": \"Python conversion script if needed\"\n",
        "                }}\n",
        "            }}\n",
        "        }}\n",
        "        ```\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    display(Markdown(\"### \"+ \"Data Scale Types\"))\n",
        "\n",
        "    # Create the Chain\n",
        "    identify_data_scale_chain = LLMChain(\n",
        "        llm=llm,\n",
        "        prompt=identify_data_scale_prompt,\n",
        "        output_key=\"data_scale_analysis\"\n",
        "    )\n",
        "\n",
        "    # Run the Chain\n",
        "    data_scale_result = identify_data_scale_chain.invoke({\n",
        "        \"df_info\": df_info_str,\n",
        "        \"df_head\": df_head_str,\n",
        "        \"df_summary\": df_summary_str,\n",
        "        \"insight_need_type\": insight_need_type,\n",
        "        \"key_variables\": key_variables\n",
        "    })\n",
        "\n",
        "    # Extract the raw JSON output\n",
        "    chain2_output_str = data_scale_result[\"data_scale_analysis\"]\n",
        "    display(Markdown(chain2_output_str))\n",
        "\n",
        "    return chain2_output_str"
      ],
      "metadata": {
        "id": "1IFggjw0qDBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### post-process data if needed"
      ],
      "metadata": {
        "id": "qf6zH9CuqEgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def post_process_data(df, chain2_output_str):\n",
        "    \"\"\"\n",
        "    Performs post-processing on the dataset by cleaning and executing LLM-generated conversion scripts.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The original dataset.\n",
        "        chain2_output_str (str): JSON string output from the LLM, containing variable classifications and conversions.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (df_filtered, column_data_types)\n",
        "              - df_filtered: DataFrame containing only key variables after conversions.\n",
        "              - column_data_types: Dictionary mapping variables to their suggested Python data types.\n",
        "    \"\"\"\n",
        "\n",
        "    # 🔹 Step 1: Print the original DataFrame types before processing\n",
        "    display(Markdown(\"### \"+ \"Data Pre-Processing:\"))\n",
        "    display(Markdown(\"Data before processing: \\n \"))\n",
        "    display(Markdown(\"```\\n\" + df.dtypes.to_string() + \"\\n```\"))\n",
        "\n",
        "    # 🔹 Step 2: Clean the LLM output before parsing JSON\n",
        "    cleaned_output = re.sub(r\"json\", \"\", chain2_output_str)  # Remove \"json\" keyword if present\n",
        "    cleaned_output = re.sub(r\"```\", \"\", cleaned_output)  # Remove code block markers\n",
        "\n",
        "    # Convert cleaned JSON string into a dictionary\n",
        "    data_scale_analysis = json.loads(cleaned_output)\n",
        "\n",
        "    # 🔹 Step 5: Apply conversions using `exec()`\n",
        "    for var, details in data_scale_analysis[\"categorized_variables\"].items():\n",
        "        if details[\"conversion_needed\"] and details[\"conversion_code\"]:\n",
        "            # print(f\"\\nExecuting conversion for {var}: {details['conversion_code']}\\n\")\n",
        "            exec(details[\"conversion_code\"])  # Executes the conversion script\n",
        "\n",
        "    # 🔹 Step 3: Extract key variables from the LLM output\n",
        "    key_variables = list(data_scale_analysis[\"categorized_variables\"].keys())\n",
        "\n",
        "    # 🔹 Step 4: Filter the DataFrame to only include key variables\n",
        "    df_filtered = df[key_variables]\n",
        "\n",
        "    # 🔹 Step 6: Verify if conversion was successful\n",
        "    # display(Markdown(print(\"\\n\\n Data after processing: \\n \",(df_filtered.dtypes))))\n",
        "    display(Markdown(\"Data after processing: \\n \"))\n",
        "    display(Markdown(\"```\\n\" + df_filtered.dtypes.to_string() + \"```\"))\n",
        "\n",
        "    # Create a dictionary mapping variables to their suggested Python data types\n",
        "    column_data_types = {\n",
        "        column: details[\"suggested_python_dtype\"]\n",
        "        for column, details in data_scale_analysis[\"categorized_variables\"].items()\n",
        "    }\n",
        "\n",
        "    return df_filtered, column_data_types"
      ],
      "metadata": {
        "id": "E1Y7moZSqDeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### choose best analysis type"
      ],
      "metadata": {
        "id": "n8yUVRJ4qy68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def select_best_analysis_type(user_request, insight_json, df_filtered, column_data_types, llm):\n",
        "    \"\"\"\n",
        "    Determines the best study type (Temporal, Geospatial, Topical, or Network) based on the dataset and visualization request.\n",
        "\n",
        "    Args:\n",
        "        user_request (str): The user's request for a visualization.\n",
        "        insight_json (dict): The dictionary containing insight need type.\n",
        "        df_filtered (pd.DataFrame): The processed dataset containing only key variables.\n",
        "        column_data_types (dict): Dictionary mapping key variables to their data types.\n",
        "        llm (LLM): The Language Model to process the request.\n",
        "\n",
        "    Returns:\n",
        "        dict: A structured dictionary containing:\n",
        "              - \"chosen_study_type\": The best study type (Temporal, Geospatial, Topical, or Network).\n",
        "              - \"reasoning\": Explanation of why this study type was chosen.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the Prompt Template\n",
        "    select_study_type_prompt = PromptTemplate(\n",
        "        input_variables=[\n",
        "            \"user_request\",\n",
        "            \"insight_need_type\",\n",
        "            \"df_info\",\n",
        "            \"df_head\",\n",
        "            \"key_variable_types\"\n",
        "        ],\n",
        "        template=\"\"\"You are an expert in data-driven analysis and visualization. Your task is to determine the **most appropriate type of study** (Temporal, Geospatial, Topical, or Network) based on the given dataset, user request and insight need type.\n",
        "\n",
        "    ---\n",
        "    ### **Study Types Overview**\n",
        "\n",
        "    #### **1. Temporal Study (\"When\")**\n",
        "    - **Purpose:** Answers questions related to time-based trends, events, bursts, or seasonality.\n",
        "    - **Typical Data:** Time-series data, timestamped records.\n",
        "    - **Key Visualizations:** Line graphs, stacked bar graphs, time histograms, flow maps.\n",
        "    - **Common Applications:** Stock market trends, sales over time, publication frequencies.\n",
        "\n",
        "    #### **2. Geospatial Study (\"Where\")**\n",
        "    - **Purpose:** Analyzes spatial distributions and relationships based on location data.\n",
        "    - **Typical Data:** Latitude/longitude, address-based data, regional statistics.\n",
        "    - **Key Visualizations:** Choropleth maps, proportional symbol maps, cartograms.\n",
        "    - **Common Applications:** Disease spread, migration patterns, resource allocation.\n",
        "\n",
        "    #### **3. Topical Study (\"What\")**\n",
        "    - **Purpose:** Analyzes text data to identify key topics, term frequencies, or content trends.\n",
        "    - **Typical Data:** Large text corpora (books, articles, papers), word frequency distributions.\n",
        "    - **Key Visualizations:** Word clouds, stream graphs, self-organizing maps.\n",
        "    - **Common Applications:** Sentiment analysis, keyword tracking, academic research trends.\n",
        "\n",
        "    #### **4. Network Study (\"With Whom\")**\n",
        "    - **Purpose:** Examines relationships, collaborations, and interactions between entities.\n",
        "    - **Typical Data:** Node-edge datasets, social networks, organizational structures.\n",
        "    - **Key Visualizations:** Node-link diagrams, Sankey diagrams, adjacency matrices.\n",
        "    - **Common Applications:** Social network analysis, knowledge graphs, citation networks.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### **User Request**\n",
        "    - **Requested Analysis:** \"{user_request}\"\n",
        "\n",
        "    ### **Insight Need Type**\n",
        "    - **Insight Needed:** {insight_need_type}\n",
        "\n",
        "    ### **Dataset Overview**\n",
        "    - **DataFrame Info:**\n",
        "      {df_info}\n",
        "    - **Sample Data:**\n",
        "      {df_head}\n",
        "\n",
        "    ### **Key Variables and Their Types**\n",
        "    - {key_variable_types}\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### **Your Task**\n",
        "    1 **Analyze** the user request, insight need type and dataset structure.\n",
        "    2 **Determine** which of the four study types (Temporal, Geospatial, Topical, or Network) best fits the given data and analysis requirements.\n",
        "    3 **Return** the study type and provide a brief justification.\n",
        "\n",
        "    ---\n",
        "\n",
        "    **Expected Output Format (JSON)**\n",
        "    ```json\n",
        "    {{\n",
        "        \"chosen_study_type\": \"Temporal / Geospatial / Topical / Network\",\n",
        "        \"reasoning\": \"Brief explanation of why this study type was chosen based on the dataset and user request.\"\n",
        "    }}\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    )\n",
        "\n",
        "    # Create the Chain\n",
        "    select_study_type_chain = LLMChain(\n",
        "        llm=llm,\n",
        "        prompt=select_study_type_prompt,\n",
        "        output_key=\"study_type_selection\"\n",
        "    )\n",
        "\n",
        "    # Run the Chain\n",
        "    study_type_result = select_study_type_chain.invoke({\n",
        "        \"user_request\": user_request,\n",
        "        \"insight_need_type\": insight_json[\"insight_need_type\"],\n",
        "        \"df_info\": df_filtered.dtypes.to_string(),  # Convert DataFrame info to string\n",
        "        \"df_head\": str(df_filtered.head()),  # Convert DataFrame head to string\n",
        "        \"key_variable_types\": column_data_types\n",
        "    })\n",
        "\n",
        "    # Extract and clean the output\n",
        "    study_type_choice = study_type_result[\"study_type_selection\"]\n",
        "\n",
        "    # Remove unnecessary formatting\n",
        "    cleaned_output = re.sub(r\"json\", \"\", study_type_choice)\n",
        "    cleaned_output = re.sub(r\"```\", \"\", cleaned_output)\n",
        "\n",
        "    # Convert cleaned JSON string into a dictionary\n",
        "    study_type_analysis = json.loads(cleaned_output)\n",
        "\n",
        "    # Extract and display the chosen study type\n",
        "    chosen_study_type = study_type_analysis[\"chosen_study_type\"]\n",
        "    display(Markdown(\"### \" + \"Chosen Analysis Type\"))\n",
        "    display(Markdown(chosen_study_type))\n",
        "    display(Markdown(study_type_analysis[\"reasoning\"]))\n",
        "\n",
        "    return study_type_analysis"
      ],
      "metadata": {
        "id": "EGu4KiUyqk8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### choose best visualization type"
      ],
      "metadata": {
        "id": "mhKQ7S2lqOaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def choose_best_visualization(user_request, insight_json, df_filtered, analysis_type, llm):\n",
        "    \"\"\"\n",
        "    Determines the best visualization type for a given dataset and user request.\n",
        "\n",
        "    Args:\n",
        "        user_request (str): The user's request for a visualization.\n",
        "        insight_json (dict): The dictionary containing insight need type.\n",
        "        df_filtered (pd.DataFrame): The processed dataset containing only key variables.\n",
        "        analysis_type (str): The chosen analysis type (Temporal, Geospatial, Topical, or Network).\n",
        "        llm (LLM): The Language Model to process the request.\n",
        "\n",
        "    Returns:\n",
        "        dict: A structured dictionary containing:\n",
        "              - \"chosen_visualization\": The best visualization type.\n",
        "              - \"reasoning\": Explanation of why this visualization was chosen.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the Prompt Template\n",
        "    choose_visualization_prompt = PromptTemplate(\n",
        "        input_variables=[\"user_request\", \"insight_need_type\", \"df_info\", \"df_head\", \"analysis_type\"],\n",
        "        template=\"\"\"\n",
        "        You are an expert in data visualization. Your task is to analyze the given data, user request, insight need type, the analysis type and select the most appropriate visualization type.\n",
        "\n",
        "        Below is a structured reference for different **visualization types** and their appropriate use cases:\n",
        "        ---\n",
        "        **Visualization Types and Their Purpose**\n",
        "\n",
        "        **Tables**\n",
        "        Tables are structured grids consisting of rows and columns where each cell contains a data value.\n",
        "        They are commonly used for structured data representation and can include frequency distributions,\n",
        "        percentages, and summary statistics. Tables support sorting, grouping, filtering, and interactive selection,\n",
        "        making them effective for displaying raw or aggregated numerical and categorical data.\n",
        "\n",
        "        **Charts**\n",
        "        Charts provide a visual representation of data without a strict reference system.\n",
        "        They are widely used to depict quantitative and qualitative relationships in an easily interpretable format.\n",
        "        Charts can be **comparative (bar charts), proportional (pie charts), or relational (bubble charts)**.\n",
        "        They can also use **size, color, and position** to encode additional data properties.\n",
        "\n",
        "        **Graphs**\n",
        "        Graphs use a coordinate system to represent relationships between variables.\n",
        "        They provide a structured way to analyze **trends, correlations, and distributions**\n",
        "        by mapping data points to a well-defined axis.\n",
        "        Graphs allow for advanced binning, interpolation, and smoothing to improve data interpretation.\n",
        "        They are particularly useful for **time-series data, correlations, and multivariate analysis**.\n",
        "\n",
        "        **Maps**\n",
        "        Maps link data to **geographical locations** and are used to show spatial distributions of data.\n",
        "        They encode additional data variables using **color, size, and patterns**,\n",
        "        making them effective for **geospatial analysis, density estimation, and regional comparisons**.\n",
        "\n",
        "        **Network Layouts**\n",
        "        Network visualizations are used to represent **relationships between data entities**.\n",
        "        They consist of **nodes (objects) and edges (connections)**, which may be directed, weighted, or hierarchical.\n",
        "        These layouts are useful for understanding **complex interactions, dependencies, and clustering in datasets**.\n",
        "        ---\n",
        "\n",
        "        ### **User Request**\n",
        "        The user wants to visualize: \"{user_request}\"\n",
        "\n",
        "        ### **Insight Need Type**\n",
        "        - {insight_need_type}\n",
        "\n",
        "        ### **Dataset Overview**\n",
        "        The dataset after processing the key variables is summarized as follows:\n",
        "\n",
        "        **DataFrame Info:**\n",
        "        {df_info}\n",
        "\n",
        "        **DataFrame Sample Rows:**\n",
        "        {df_head}\n",
        "\n",
        "        ### **Chosen Analysis Type**\n",
        "        The analysis will use:\n",
        "        {analysis_type}\n",
        "\n",
        "        ### **Your Task**\n",
        "        - **Step 1:** Analyze the **user request**, the **insight need type** and the **analysis type**.\n",
        "        - **Step 2:** Review the **data structure** from the `df_info()` and `df_head()` output.\n",
        "        - **Step 3:** Choose the **best visualization type** from the provided reference based on:\n",
        "            - The **type of data** (numerical, categorical, temporal, geospatial, relational).\n",
        "            - The **insight need type** (e.g., trends, distributions, correlations, comparisons).\n",
        "            - The **user’s intended goal** (e.g., exploratory analysis, detailed reporting, summarization).\n",
        "            - The **analysis type** (e.g., temporal, geospatial, topical and network).\n",
        "\n",
        "        **Expected Output Format (JSON)**\n",
        "        ```json\n",
        "        {{\n",
        "            \"chosen_visualization\": \"Selected Visualization Type\",\n",
        "            \"reasoning\": \"Brief explanation of why this visualization was chosen based on the data and insight need.\"\n",
        "        }}\n",
        "        ```\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Create the Chain\n",
        "    choose_visualization_chain = LLMChain(\n",
        "        llm=llm,\n",
        "        prompt=choose_visualization_prompt,\n",
        "        output_key=\"visualization_choice\"\n",
        "    )\n",
        "\n",
        "    # Run the Chain\n",
        "    visualization_result = choose_visualization_chain.invoke({\n",
        "        \"user_request\": user_request,\n",
        "        \"insight_need_type\": insight_json[\"insight_need_type\"],\n",
        "        \"df_info\": str(df_filtered.dtypes.to_string()),  # Convert DataFrame info to string\n",
        "        \"df_head\": str(df_filtered.head()),   # Convert DataFrame head to string,\n",
        "        \"analysis_type\": analysis_type\n",
        "    })\n",
        "\n",
        "    # Extract and clean the output\n",
        "    visualization_choice = visualization_result[\"visualization_choice\"]\n",
        "\n",
        "    # Remove unnecessary formatting\n",
        "    cleaned_output = re.sub(r\"json\", \"\", visualization_choice)\n",
        "    cleaned_output = re.sub(r\"```\", \"\", cleaned_output)\n",
        "\n",
        "    # Convert cleaned JSON string into a dictionary\n",
        "    visualization_data = json.loads(cleaned_output)\n",
        "\n",
        "    # Extract and display the chosen visualization type\n",
        "    chosen_visualization_type = visualization_data[\"chosen_visualization\"]\n",
        "    print(\"\\n\\n\")\n",
        "    display(Markdown(\"### \"+ \"Chosen Visualization Type\"))\n",
        "    display(Markdown(chosen_visualization_type))\n",
        "    display(Markdown(visualization_data['reasoning']))\n",
        "\n",
        "    return visualization_data"
      ],
      "metadata": {
        "id": "A64QK3EYqNzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### choose graphic symbols"
      ],
      "metadata": {
        "id": "dtIq6xl8qZkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def choose_best_graphic_symbol(user_request, insight_json, df_filtered, column_data_types, chosen_visualization_type, analysis_type, llm):\n",
        "    \"\"\"\n",
        "    Determines the best graphic symbol type for a given dataset and user request.\n",
        "\n",
        "    Args:\n",
        "        user_request (str): The user's request for a visualization.\n",
        "        insight_json (dict): The dictionary containing insight need type.\n",
        "        df_filtered (pd.DataFrame): The processed dataset containing only key variables.\n",
        "        column_data_types (dict): Dictionary mapping key variables to their data types.\n",
        "        chosen_visualization_type (str): The chosen visualization type from the previous step.\n",
        "        analysis_type (str): The chosen analysis type (Temporal, Geospatial, Topical, or Network).\n",
        "        llm (LLM): The Language Model to process the request.\n",
        "\n",
        "    Returns:\n",
        "        dict: A structured dictionary containing:\n",
        "              - \"chosen_graphic_symbol\": The best graphic symbol type.\n",
        "              - \"reasoning\": Explanation of why this symbol was chosen.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the Prompt Template\n",
        "    choose_graphic_symbol_prompt = PromptTemplate(\n",
        "        input_variables=[\n",
        "            \"user_request\", \"insight_need_type\",\n",
        "            \"df_info\", \"df_head\", \"key_variable_types\", \"chosen_visualization_type\", \"analysis_type\"\n",
        "        ],\n",
        "        template=\"\"\"\n",
        "    You are an expert in data visualization and graphic design. Your task is to analyze the dataset, user request, insight need type, analysis type and visualization type to determine the **most appropriate graphic symbol** to represent the data.\n",
        "\n",
        "    Below is a structured reference on **graphic symbol types** and their appropriate use cases:\n",
        "    ---\n",
        "    Graphic symbols are geometric elements used to represent data records in visualizations. They encode different data variables using graphic variable types such as position, size, color, and shape. These symbols help identify, differentiate, and structure information effectively.\n",
        "\n",
        "    Types of Graphic Symbols\n",
        "\n",
        "1. Geometric Symbols\n",
        "Geometric symbols are defined by their dimensionality and include:\n",
        "\t•\tPoints: Represent discrete locations on a map or scatter plot.\n",
        "\t•\tLines: Connect points and are used to denote relationships, movement, or trends (e.g., line graphs, network visualizations).\n",
        "\t•\tAreas: Enclose a region to show boundaries, density, or proportions (e.g., choropleth maps).\n",
        "\t•\tSurfaces: Represent continuous data fields over a 2D space (e.g., heatmaps, topographic maps).\n",
        "\t•\tVolumes: Three-dimensional representations for depth-related data (e.g., 3D bar charts, terrain models).\n",
        "\n",
        "Use Cases:\n",
        "\t•\tPoints: Scatter plots, maps, proportional symbol maps.\n",
        "\t•\tLines: Line graphs, network diagrams, flow maps.\n",
        "\t•\tAreas: Choropleth maps, stacked area charts.\n",
        "\t•\tSurfaces: Heatmaps, 3D terrain maps.\n",
        "\t•\tVolumes: 3D bar charts, volumetric visualizations.\n",
        "\n",
        "2. Linguistic Symbols\n",
        "Linguistic symbols use letters, numbers, and punctuation to represent data. These include:\n",
        "\t•\tLabels & Text: Used for axis labels, map annotations, or word clouds.\n",
        "\t•\tNumeric Values: Data encoding through formatted numbers.\n",
        "\t•\tWords & Sentences: Text-based data representations in infographics.\n",
        "\n",
        "Use Cases:\n",
        "\t•\tText labels: Titles, axis labels, data callouts.\n",
        "\t•\tWords & Numbers: Word clouds, tag clouds, typographic maps.\n",
        "\t•\tProportional Typeface: Font size encoding (e.g., larger text for higher values).\n",
        "\n",
        "3. Pictorial Symbols\n",
        "Pictorial symbols are visual representations of objects used to improve interpretation:\n",
        "\t•\tIcons & Images: Simple visual markers (e.g., airplane icon for airports).\n",
        "\t•\tSilhouettes & Profiles: Abstract depictions of categories (e.g., gender icons).\n",
        "\t•\tProportional Pictograms: Icon sizes represent numerical values.\n",
        "\n",
        "Use Cases:\n",
        "\t•\tIcons: Maps, dashboards, infographics.\n",
        "\t•\tSilhouettes: Demographic visuals, symbolic representations.\n",
        "\t•\tProportional Pictograms: Population pyramids, gender-based visualizations.\n",
        "\n",
        "4. Statistical Glyphs\n",
        "Statistical glyphs are compact visual representations of data, often using:\n",
        "\t•\tChernoff Faces: Encodes multiple data variables into facial expressions.\n",
        "\t•\tSmall Multiples: Repeated small visualizations for comparison.\n",
        "\t•\tData Dots & Bar Glyphs: Encodes quantitative values using length or density.\n",
        "\n",
        "Use Cases:\n",
        "\t•\tChernoff Faces: Multivariate human perception studies.\n",
        "\t•\tSmall Multiples: Comparative analysis (e.g., crime rates by region).\n",
        "\t•\tBar Glyphs: Encoded bar-based data for visual quick reading.\n",
        "\n",
        "Combinations of Graphic Symbols\n",
        "Graphic symbols can be combined to enhance visualization:\n",
        "\t•\tGeometric + Linguistic Symbols: Word clouds overlaid on maps.\n",
        "\t•\tPictorial + Statistical Glyphs: Infographics with pictogram-based charts.\n",
        "\t•\tMultiple Graphic Variables: Encoding size, color, and shape simultaneously.\n",
        "\n",
        "How to Choose a Graphic Symbol for Data Variables\n",
        "\t•\tIf the data is spatial/geographical, use Points, Lines, or Areas.\n",
        "\t•\tIf the data involves text-based representation, use Linguistic Symbols.\n",
        "\t•\tIf the visualization requires real-world object representation, use Pictorial Symbols.\n",
        "\t•\tIf the data needs compact statistical representation, use Statistical Glyphs.\n",
        "\t•\tIf multiple dimensions need to be encoded visually, combine symbols.\n",
        "\n",
        "    ---\n",
        "\n",
        "    ### **User Request**\n",
        "    The user wants to visualize: \"{user_request}\"\n",
        "\n",
        "    ### **Insight Need Type**\n",
        "    - {insight_need_type}\n",
        "\n",
        "    ### **Analysis Type**\n",
        "    - {analysis_type}\n",
        "\n",
        "    ### **Chosen Visualization Type**\n",
        "    The visualization will use:\n",
        "    {chosen_visualization_type}\n",
        "\n",
        "    ### **Dataset Overview**\n",
        "    The dataset after processing the key variables is summarized as follows:\n",
        "\n",
        "    **DataFrame Info:**\n",
        "    {df_info}\n",
        "\n",
        "    **DataFrame Sample Rows:**\n",
        "    {df_head}\n",
        "\n",
        "    ### **Key Variable Data Types**\n",
        "    The data types of key variables after processing:\n",
        "    {key_variable_types}\n",
        "\n",
        "    ### **Your Task**\n",
        "    - **Step 1:** Analyze the **user request**, the **insight need type** the **chosen analysis type** and the **chosen visualization type**.\n",
        "    - **Step 2:** Review the **data structure** from the `df_info()` and `df_head()` output.\n",
        "    - **Step 3:** Examine the **data types of key variables** to determine the best graphic symbol type.\n",
        "    - **Step 4:** Choose the **best graphic symbol** from the provided reference based on:\n",
        "        - The **type of data** (categorical, numerical, ordinal, geospatial, network).\n",
        "        - The **insight need type** (e.g., trends, distributions, correlations, comparisons).\n",
        "        - The **user’s intended goal** (e.g., exploratory analysis, detailed reporting, summarization).\n",
        "        - The **analysis type** (e.g., temporal, geospatial, topical and network).\n",
        "        - The **chosen visualization type** (e.g., scatter plot, choropleth map).\n",
        "\n",
        "    **Expected Output Format (JSON)**\n",
        "    ```json\n",
        "    {{\n",
        "        \"chosen_graphic_symbol\": \"Selected Graphic Symbol\",\n",
        "        \"reasoning\": \"Brief explanation of why this symbol was chosen based on the data, insight need, and variable types.\"\n",
        "    }}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    # Create the Chain\n",
        "    choose_graphic_symbol_chain = LLMChain(\n",
        "        llm=llm,\n",
        "        prompt=choose_graphic_symbol_prompt,\n",
        "        output_key=\"graphic_symbol_choice\"\n",
        "    )\n",
        "\n",
        "    # Run the Chain\n",
        "    graphic_symbol_result = choose_graphic_symbol_chain.invoke({\n",
        "        \"user_request\": user_request,\n",
        "        \"insight_need_type\": insight_json[\"insight_need_type\"],\n",
        "        \"df_info\": df_filtered.dtypes.to_string(),  # Convert DataFrame info to string\n",
        "        \"df_head\": str(df_filtered.head()),  # Convert DataFrame head to string\n",
        "        \"key_variable_types\": column_data_types,  # JSON output from post-processing\n",
        "        \"chosen_visualization_type\": chosen_visualization_type,  # Output from visualization choice\n",
        "        \"analysis_type\": analysis_type  # Output from analysis choice\n",
        "    })\n",
        "\n",
        "    # Extract and clean the output\n",
        "    graphic_symbol_choice = graphic_symbol_result[\"graphic_symbol_choice\"]\n",
        "\n",
        "    # Remove unnecessary formatting\n",
        "    cleaned_output = re.sub(r\"json\", \"\", graphic_symbol_choice)\n",
        "    cleaned_output = re.sub(r\"```\", \"\", cleaned_output)\n",
        "\n",
        "    # Convert cleaned JSON string into a dictionary\n",
        "    graphic_symbol_data = json.loads(cleaned_output)\n",
        "\n",
        "    # Extract and display the chosen graphic symbol type\n",
        "    chosen_graphic_symbol = graphic_symbol_data[\"chosen_graphic_symbol\"]\n",
        "    graphic_symbol_explanation = graphic_symbol_data[\"reasoning\"]\n",
        "    display(Markdown(\"### \" + \"Chosen Graphic Symbol\"))\n",
        "    display(Markdown(chosen_graphic_symbol))\n",
        "    display(Markdown(graphic_symbol_explanation))\n",
        "\n",
        "    return graphic_symbol_data"
      ],
      "metadata": {
        "id": "MjNsXXKZqVlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### choose graphic variable data"
      ],
      "metadata": {
        "id": "VgzimnGUqggK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_best_graphic_variable(user_request, insight_json, df_filtered, column_data_types, chosen_graphic_symbol, chosen_visualization_type, chosen_analysis_type, llm):\n",
        "    \"\"\"\n",
        "    Determines the best graphic variable type for a given dataset and user request.\n",
        "\n",
        "    Args:\n",
        "        user_request (str): The user's request for a visualization.\n",
        "        insight_json (dict): The dictionary containing insight need type.\n",
        "        df_filtered (pd.DataFrame): The processed dataset containing only key variables.\n",
        "        column_data_types (dict): Dictionary mapping key variables to their data types.\n",
        "        chosen_graphic_symbol (str): The chosen graphic symbol from the previous step.\n",
        "        chosen_visualization_type (str): The chosen visualization type from the previous step.\n",
        "        chosen_analysis_type (str): The chosen analysis type (Temporal, Geospatial, Topical, or Network).\n",
        "        llm (LLM): The Language Model to process the request.\n",
        "\n",
        "    Returns:\n",
        "        dict: A structured dictionary containing:\n",
        "              - \"chosen_graphic_variable\": The best graphic variable(s).\n",
        "              - \"reasoning\": Explanation of why this variable was chosen.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the Prompt Template\n",
        "    choose_graphic_variable_prompt = PromptTemplate(\n",
        "        input_variables=[\n",
        "            \"user_request\", \"insight_need_type\",\n",
        "            \"df_info\", \"df_head\", \"key_variable_types\", \"chosen_graphic_symbol\", \"chosen_visualization_type\", \"chosen_analysis_type\"\n",
        "        ],\n",
        "        template=\"\"\"\n",
        "    You are an expert in data visualization and perceptual design.\n",
        "    Your task is to analyze the dataset, user request, insight need type, chosen analysis type, chosen visualization type and chosen graphic symbol to determine the **most effective graphic variable(s)** to visually encode the data.\n",
        "\n",
        "    Below is a structured reference on **graphic variable types** and their appropriate use cases:\n",
        "    ---\n",
        "    Graphic variable types are visual properties used to encode data values in a visualization. These variables modify graphic symbols (such as points, lines, areas, pictorial symbols, or statistical glyphs) to enhance their expressiveness. They help define relationships, differentiate between elements, and improve interpretability.\n",
        "\n",
        "    Types of Graphic Variables and Their Functions\n",
        "\n",
        "1. Spatial Variables (Position & Location)\n",
        "\t•\tDefinition: Specifies where an element is placed in 2D or 3D space.\n",
        "\t•\tUsage: Most effective for encoding quantitative data since human perception excels at detecting relative positioning.\n",
        "\t•\tExamples: Scatter plots, maps, network layouts.\n",
        "\n",
        "2. Retinal Variables (Used for Non-Spatial Encodings)\n",
        "These variables modify appearance properties of symbols without changing their spatial position.\n",
        "\n",
        "2.1 Size\n",
        "\t•\tDefinition: Controls relative magnitude of a symbol (small vs. large).\n",
        "\t•\tUsage: Best suited for quantitative comparisons (e.g., bubble charts, proportional symbol maps).\n",
        "\t•\tLimitations: Size perception is non-linear; small differences are harder to distinguish.\n",
        "\n",
        "2.2 Shape\n",
        "\t•\tDefinition: Differentiates categories using distinct geometric forms (e.g., circles, triangles, squares).\n",
        "\t•\tUsage: Best for categorical (nominal) data, where distinct symbols aid identification.\n",
        "\t•\tExamples: Scatter plots with different markers for categories.\n",
        "\n",
        "2.3 Orientation\n",
        "\t•\tDefinition: Adjusts rotation angle of a symbol (e.g., tilted lines).\n",
        "\t•\tUsage: Occasionally used for ordinal or directional data (e.g., wind direction in weather maps).\n",
        "\t•\tLimitations: Humans struggle to distinguish small rotation differences.\n",
        "\n",
        "2.4 Color (Hue & Value)\n",
        "\t•\tHue (Categorical Encoding):\n",
        "\t•\tEncodes qualitative (nominal) categories using distinct colors (e.g., red = emergency, blue = calm).\n",
        "\t•\tBest used for: Grouping elements without implying order.\n",
        "\t•\tValue (Lightness/Darkness for Quantitative Encoding):\n",
        "\t•\tEncodes sequential or ordinal values (lighter = lower value, darker = higher value).\n",
        "\t•\tBest used for: Choropleth maps, heatmaps, density plots.\n",
        "\t•\tSaturation (Intensity):\n",
        "\t•\tIndicates importance or uncertainty (e.g., faded color suggests missing or unreliable data).\n",
        "\n",
        "2.5 Texture & Pattern\n",
        "\t•\tDefinition: Uses repeating marks, dots, or grid patterns to encode information.\n",
        "\t•\tUsage: Best for distinguishing overlapping elements (e.g., land use maps with different textures).\n",
        "\n",
        "2.6 Transparency\n",
        "\t•\tDefinition: Alters opacity to indicate uncertainty, density, or layering.\n",
        "\t•\tUsage: Frequently used in scatterplots with high overlap and uncertainty visualizations.\n",
        "\n",
        "2.7 Depth (3D Perspective)\n",
        "\t•\tDefinition: Simulates perspective depth in 3D visualizations.\n",
        "\t•\tUsage: Used in 3D bar charts, terrain models, and volumetric visualizations.\n",
        "\t•\tLimitations: Can cause occlusion issues (one element hiding another).\n",
        "\n",
        "2.8 Blur & Optics\n",
        "\t•\tDefinition: Adds blur or focus to control emphasis.\n",
        "\t•\tUsage: Highlights important elements while fading others (e.g., blurred background in network layouts).\n",
        "\n",
        "2.9 Motion & Animation\n",
        "\t•\tDefinition: Uses movement speed, direction, and rhythm to encode information.\n",
        "\t•\tUsage: Best for time-series visualizations, transitions, and attention guidance (e.g., animated flow maps).\n",
        "    ---\n",
        "\n",
        "    ### **User Request**\n",
        "    The user wants to visualize: \"{user_request}\"\n",
        "\n",
        "    ### **Insight Need Type**\n",
        "    - {insight_need_type}\n",
        "\n",
        "    ### **Analysis Type**\n",
        "    - {chosen_analysis_type}\n",
        "\n",
        "    ### **Chosen Visualization Type**\n",
        "    The visualization will use:\n",
        "    {chosen_visualization_type}\n",
        "\n",
        "    ### **Dataset Overview**\n",
        "    The dataset after processing the key variables is summarized as follows:\n",
        "\n",
        "    **DataFrame Info:**\n",
        "    {df_info}\n",
        "\n",
        "    **DataFrame Sample Rows:**\n",
        "    {df_head}\n",
        "\n",
        "    ### **Key Variable Data Types**\n",
        "    The data types of key variables after processing:\n",
        "    {key_variable_types}\n",
        "\n",
        "    ### **Chosen Graphic Symbol**\n",
        "    The visualization will use: **{chosen_graphic_symbol}**\n",
        "\n",
        "    ### **Your Task**\n",
        "    - **Step 1:** Analyze the **user request** and the **insight need type**, the **chosen analysis type**, the **chosen visualization type** and the **chosen graphic symbol**.\n",
        "    - **Step 2:** Review the **data structure** from the `df_info()` and `df_head()` output.\n",
        "    - **Step 3:** Examine the **data types of key variables** to determine the best graphic variable.\n",
        "    - **Step 4:** Choose the **best graphic variable(s)** from the provided reference based on:\n",
        "        - The **type of data** (categorical, numerical, ordinal, spatial, network).\n",
        "        - The **insight need type** (e.g., trends, distributions, correlations, comparisons).\n",
        "        - The **chosen graphic symbol** and how best to visually encode it.\n",
        "    - **Step 5:** If needed, suggest **multiple graphic variables** (e.g., **color + size** for dual encoding).\n",
        "\n",
        "    **Expected Output Format (JSON)**\n",
        "    ```json\n",
        "    {{\n",
        "        \"chosen_graphic_variable\": \"Selected Graphic Variable(s)\",\n",
        "        \"reasoning\": \"Brief explanation of why this variable was chosen based on the data, visualization type, and perceptual principles.\"\n",
        "    }}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    # Create the Chain\n",
        "    choose_graphic_variable_chain = LLMChain(\n",
        "        llm=llm,\n",
        "        prompt=choose_graphic_variable_prompt,\n",
        "        output_key=\"graphic_variable_choice\"\n",
        "    )\n",
        "\n",
        "    # Run the Chain\n",
        "    graphic_variable_result = choose_graphic_variable_chain.invoke({\n",
        "        \"user_request\": user_request,\n",
        "        \"insight_need_type\": insight_json[\"insight_need_type\"],\n",
        "        \"df_info\": df_filtered.dtypes.to_string(),  # Convert DataFrame info to string\n",
        "        \"df_head\": str(df_filtered.head()),  # Convert DataFrame head to string\n",
        "        \"key_variable_types\": column_data_types,  # JSON output from post-processing\n",
        "        \"chosen_graphic_symbol\": chosen_graphic_symbol,  # Output from previous chain\n",
        "        \"chosen_visualization_type\": chosen_visualization_type,  # Output from visualization choice\n",
        "        \"chosen_analysis_type\": chosen_analysis_type  # Output from analysis choice\n",
        "    })\n",
        "\n",
        "    # Extract and clean the output\n",
        "    graphic_variable_choice = graphic_variable_result[\"graphic_variable_choice\"]\n",
        "\n",
        "    # Remove unnecessary formatting\n",
        "    cleaned_output = re.sub(r\"json\", \"\", graphic_variable_choice)\n",
        "    cleaned_output = re.sub(r\"```\", \"\", cleaned_output)\n",
        "\n",
        "    # Convert cleaned JSON string into a dictionary\n",
        "    graphic_variable_data = json.loads(cleaned_output)\n",
        "\n",
        "    # Extract and display the chosen graphic variable type\n",
        "    chosen_graphic_variable = graphic_variable_data[\"chosen_graphic_variable\"]\n",
        "    graphic_variable_explanation = graphic_variable_data[\"reasoning\"]\n",
        "    display(Markdown(\"### \" + \"Chosen Graphic Variable\"))\n",
        "    display(Markdown(chosen_graphic_variable))\n",
        "    display(Markdown(graphic_variable_explanation))\n",
        "\n",
        "    return graphic_variable_data"
      ],
      "metadata": {
        "id": "iU9hyzWmqb70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### execute python code generated from llm"
      ],
      "metadata": {
        "id": "EgBnjqKhq351"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def execute_visualization_from_json(visualization_json):\n",
        "    \"\"\"\n",
        "    Executes the generated Python visualization code from the parsed JSON output.\n",
        "\n",
        "    Parameters:\n",
        "        visualization_json (dict): JSON dictionary containing visualization details,\n",
        "                                   including 'generated_code' for each visualization.\n",
        "\n",
        "    Returns:\n",
        "        None: The function runs the visualization code dynamically.\n",
        "    \"\"\"\n",
        "\n",
        "    for viz_key, viz_details in visualization_json.items():\n",
        "        print(f\"\\n🔹 Running: {viz_details['name']} Visualization\")\n",
        "\n",
        "        try:\n",
        "            # Extract the Python code\n",
        "            viz_code = viz_details[\"generated_code\"]\n",
        "            print(\"🔻 Python Code:\\n\", viz_code)\n",
        "\n",
        "            # Execute the visualization code safely\n",
        "            exec(viz_code, globals())\n",
        "\n",
        "            print(f\"✅ Successfully executed: {viz_details['name']}\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error while executing {viz_details['name']} Visualization\")\n",
        "            print(\"🔻 Exception Message:\", str(e))\n",
        "            print(\"🔻 Traceback:\\n\", traceback.format_exc())"
      ],
      "metadata": {
        "id": "IXV6nlUCq3OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### temporal analysis"
      ],
      "metadata": {
        "id": "FNjjPm0mrDiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_temporal_analysis_chain(\n",
        "    llm,\n",
        "    user_request,\n",
        "    insight_need_type,\n",
        "    chosen_visualization_type,\n",
        "    df_filtered,\n",
        "    chosen_graphic_symbol,\n",
        "    graphic_symbol_explanation,\n",
        "    chosen_graphic_variable,\n",
        "    graphic_variable_explanation\n",
        "):\n",
        "    \"\"\"\n",
        "    Executes the temporal analysis chain and returns the parsed JSON output.\n",
        "\n",
        "    Parameters:\n",
        "        llm: LangChain LLM instance\n",
        "        user_request (str): The user’s request for visualization.\n",
        "        insight_need_type (str): The insight requirement output from Chain 1.\n",
        "        chosen_visualization_type (str): The visualization type determined from previous chains.\n",
        "        df_filtered (pd.DataFrame): The processed dataset.\n",
        "        chosen_graphic_symbol (str): Selected graphic symbol for visualization.\n",
        "        graphic_symbol_explanation (str): Explanation for choosing the graphic symbol.\n",
        "        chosen_graphic_variable (str): Selected graphic variable for visualization.\n",
        "        graphic_variable_explanation (str): Explanation for choosing the graphic variable.\n",
        "\n",
        "    Returns:\n",
        "        dict: Parsed JSON output containing visualization recommendations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the Prompt Template\n",
        "    temporal_analysis_prompt = PromptTemplate(\n",
        "        input_variables=[\n",
        "            \"user_request\", \"insight_need_type\", \"chosen_visualization_type\", \"df_info\", \"df_head\", \"df_summary\",\n",
        "            \"chosen_graphic_symbol\", \"graphic_symbol_explanation\",\n",
        "            \"chosen_graphic_variable\", \"graphic_variable_explanation\"\n",
        "        ],\n",
        "        template=\"\"\"\n",
        "        You are a highly skilled data visualization expert specializing in temporal analysis.\n",
        "        Your goal is to analyze the provided dataset and generate the best possible **time-based visualizations**.\n",
        "\n",
        "        ---\n",
        "        ### **Temporal Studies Context**\n",
        "        Temporal Studies and Visualization Types\n",
        "\n",
        "Overview\n",
        "Temporal analysis and visualization techniques are used to answer \"When\" questions. These techniques help identify trends, bursts, seasonality, and patterns over time. Different approaches are required to manage various time-related challenges such as aggregation, time zones, outliers, and time slicing.\n",
        "\n",
        "Data Preprocessing\n",
        "A time series consists of events or observations ordered in one dimension: time. It may be:\n",
        "- Continuous: Observations recorded at equal intervals (e.g., every second).\n",
        "- Discrete: Observations recorded irregularly (e.g., event-driven logs).\n",
        "\n",
        "Temporal information can be static (fixed dates) or dynamic (real-time data streams).\n",
        "\n",
        "Resolution and Aggregation\n",
        "- Time can be expressed in milliseconds, seconds, minutes, hours, days, weeks, months, years, decades, centuries.\n",
        "- Aggregation can be based on astronomical time (e.g., years, seasons) or cultural time (e.g., fiscal years, business quarters).\n",
        "\n",
        "Time Zones\n",
        "Different regions use different time zones. Aligning data across time zones ensures correct comparisons.\n",
        "\n",
        "Outliers\n",
        "- Definition: Extreme values that deviate significantly from normal patterns.\n",
        "- Example: A sudden traffic spike on a website due to viral content.\n",
        "- Outliers can be filtered out or highlighted for deeper investigation.\n",
        "\n",
        "Time Slicing\n",
        "- Disjoint Slicing: Separates time intervals completely.\n",
        "- Overlapping Slicing: Allows segments to share part of the time range.\n",
        "- Cumulative Slicing: Accumulates data across periods (e.g., rolling averages).\n",
        "\n",
        "Temporal Trends and Patterns\n",
        "A time series can be broken down into:\n",
        "1. General Trends: Long-term progression.\n",
        "2. Cyclical Components: Recurring changes (e.g., business cycles).\n",
        "3. Seasonal Components: Repeating yearly patterns (e.g., winter sales).\n",
        "4. Random Components: Unpredictable variations.\n",
        "\n",
        "To analyze trends, smoothing techniques can be applied (e.g., moving averages).\n",
        "\n",
        "Bursts\n",
        "- Definition: A sudden increase in activity for a short period.\n",
        "- Example: An unexpected surge in search engine queries.\n",
        "- Burst detection algorithms identify and rank significant events.\n",
        "\n",
        "Temporal Visualization Types\n",
        "\n",
        "1. Trends and Distributions\n",
        "- Timeline/Chronological Graphs: Show event occurrences over time.\n",
        "- Bar Graphs: Used for discrete time segments.\n",
        "- Line Graphs: Highlight trends in continuous time series.\n",
        "- Histograms: Show frequency distributions.\n",
        "\n",
        "Example: Stock price trends over time.\n",
        "\n",
        "2. Time-Based Comparisons\n",
        "- Stacked Bar Graphs: Compare multiple datasets over time.\n",
        "- 100% Stacked Line Graphs: Show relative proportions.\n",
        "- Circular Time Graphs: Represent periodic patterns (e.g., hourly energy usage).\n",
        "\n",
        "3. Flows Over Time\n",
        "- Flow Maps: Show movement and volume of entities over time (e.g., migration patterns).\n",
        "- Space-Time Cube Maps: Represent data in three dimensions (X, Y for space, Z for time).\n",
        "\n",
        "Example: Hurricane track visualization.\n",
        "\n",
        "4. Derivatives and Change Detection\n",
        "- Velocity & Acceleration Graphs: Analyze change rates (e.g., stock market fluctuations).\n",
        "- Animated Sequences: Display data evolution over time (e.g., animated COVID-19 case maps).\n",
        "\n",
        "Application of Temporal Studies\n",
        "1. Finance: Stock price trends, inflation changes.\n",
        "2. Healthcare: Epidemic progression, patient recovery rates.\n",
        "3. Climate Science: Temperature shifts, seasonal rainfall.\n",
        "4. Social Media & Search Trends: Burst analysis, trending topics.\n",
        "\n",
        "        ---\n",
        "        ### **User Request**\n",
        "        The user wants to visualize: \"{user_request}\"\n",
        "\n",
        "        ### **Insight Need Type**\n",
        "        - {insight_need_type}\n",
        "\n",
        "        ### **Chosen Visualization Type**\n",
        "        The visualization will use:\n",
        "        {chosen_visualization_type}\n",
        "\n",
        "        ---\n",
        "        ### **Dataset Overview**\n",
        "        **DataFrame Info:**\n",
        "        {df_info}\n",
        "\n",
        "        **DataFrame Sample Rows:**\n",
        "        {df_head}\n",
        "\n",
        "        **DataFrame Summary Statistics:**\n",
        "        {df_summary}\n",
        "\n",
        "        ---\n",
        "        ### **Chosen Graphic Symbol**\n",
        "        **Symbol:** {chosen_graphic_symbol}\n",
        "        **Reasoning:** {graphic_symbol_explanation}\n",
        "\n",
        "        ### **Chosen Graphic Variable**\n",
        "        **Variable:** {chosen_graphic_variable}\n",
        "        **Reasoning:** {graphic_variable_explanation}\n",
        "\n",
        "        ---\n",
        "        ### **Your Tasks**\n",
        "        - **Step 1:** Based on the dataset structure, insight need and chosen_visualization_type, select the **best** temporal visualization.\n",
        "        - **Step 2:** For the chosen visualization, list available **software tools** that can generate it (Power BI, Tableau, Python, etc.).\n",
        "        - **Step 3:** Choose the most **suitable Python libraries** (preferably interactive ones like Plotly, Bokeh, Altair, etc.).\n",
        "        - **Step 4:** Identify any **data pre-processing** steps necessary before visualization with respect to the current dataset structure.\n",
        "        - **Step 5:** Write **Python code** that:\n",
        "            - **Uses the existing dataset (df_filtered) in memory** without generating a new sample dataset.\n",
        "            - **DO NOT create a new DataFrame**. Use `df_filtered` directly.\n",
        "            - **Processes the dataset** correctly.\n",
        "            - **Generates a high-quality visualization** with:\n",
        "              - Proper **titles, labels, legends, and data labels** for readability.\n",
        "              - Interactive features if applicable.\n",
        "            - **Outputs the visualization.**\n",
        "\n",
        "        ---\n",
        "        ### **Expected JSON Output Format**\n",
        "        ```json\n",
        "        {{\n",
        "            \"visualization_1\": {{\n",
        "                \"name\": \"Visualization Type 1\",\n",
        "                \"recommended_software\": [\"Software 1\", \"Software 2\", \"Software 3\"],\n",
        "                \"selected_libraries\": [\"Library 1\", \"Library 2\"],\n",
        "                \"preprocessing_steps\": \"Necessary preprocessing before visualization.\",\n",
        "                \"generated_code\": \"Python code for Visualization 1.\"\n",
        "            }}\n",
        "        }}\n",
        "        ```\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Create the Chain\n",
        "    temporal_analysis_chain = LLMChain(\n",
        "        llm=llm,\n",
        "        prompt=temporal_analysis_prompt,\n",
        "        output_key=\"temporal_analysis_output\"\n",
        "    )\n",
        "\n",
        "    # Execute the Chain\n",
        "    temporal_analysis_result = temporal_analysis_chain.invoke({\n",
        "        \"user_request\": user_request,\n",
        "        \"insight_need_type\": insight_need_type,\n",
        "        \"chosen_visualization_type\": chosen_visualization_type,\n",
        "        \"df_info\": df_filtered.dtypes.to_string(),\n",
        "        \"df_head\": str(df_filtered.head()),\n",
        "        \"df_summary\": str(df_filtered.describe(include='all')),\n",
        "        \"chosen_graphic_symbol\": chosen_graphic_symbol,\n",
        "        \"graphic_symbol_explanation\": graphic_symbol_explanation,\n",
        "        \"chosen_graphic_variable\": chosen_graphic_variable,\n",
        "        \"graphic_variable_explanation\": graphic_variable_explanation\n",
        "    })\n",
        "\n",
        "    # Extract and Clean Output\n",
        "    temporal_analysis_output = temporal_analysis_result[\"temporal_analysis_output\"]\n",
        "    cleaned_output = re.sub(r\"```json\", \"\", temporal_analysis_output)\n",
        "    cleaned_output = re.sub(r\"```\", \"\", cleaned_output)\n",
        "\n",
        "    # Convert cleaned JSON string into a dictionary\n",
        "    try:\n",
        "        parsed_json = json.loads(cleaned_output)\n",
        "        for key, visualization in parsed_json.items():\n",
        "            markdown_output = f\"\"\"\n",
        "        **Visualization Name**\n",
        "        **{visualization.get('name', 'N/A')}**\n",
        "\n",
        "        **Recommended Software**\n",
        "        - {', '.join(visualization.get('recommended_software', []))}\n",
        "\n",
        "        **Selected Libraries**\n",
        "        - {', '.join(visualization.get('selected_libraries', []))}\n",
        "\n",
        "        **Preprocessing Steps**\n",
        "        {visualization.get('preprocessing_steps', 'N/A')}\n",
        "        \"\"\"\n",
        "        # Display dynamically formatted Markdown\n",
        "            display(Markdown(markdown_output))\n",
        "        return parsed_json\n",
        "    except json.JSONDecodeError:\n",
        "        raise ValueError(\"Invalid JSON response from LLM. Please check the output formatting.\")"
      ],
      "metadata": {
        "id": "A9hlp1Cgq_jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### geospatial analysis"
      ],
      "metadata": {
        "id": "3-GWUD09rRJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_geospatial_analysis_chain(\n",
        "    llm,\n",
        "    user_request,\n",
        "    insight_need_type,\n",
        "    chosen_visualization_type,\n",
        "    df_filtered,\n",
        "    chosen_graphic_symbol,\n",
        "    graphic_symbol_explanation,\n",
        "    chosen_graphic_variable,\n",
        "    graphic_variable_explanation\n",
        "):\n",
        "    \"\"\"\n",
        "    Executes the geospatial analysis chain and returns the parsed JSON output.\n",
        "\n",
        "    Parameters:\n",
        "        llm: LangChain LLM instance\n",
        "        user_request (str): The user’s request for geospatial visualization.\n",
        "        insight_need_type (str): The insight requirement output from previous chains.\n",
        "        chosen_visualization_type (str): The visualization type determined from previous chains.\n",
        "        df_filtered (pd.DataFrame): The processed dataset.\n",
        "        chosen_graphic_symbol (str): Selected graphic symbol for visualization.\n",
        "        graphic_symbol_explanation (str): Explanation for choosing the graphic symbol.\n",
        "        chosen_graphic_variable (str): Selected graphic variable for visualization.\n",
        "        graphic_variable_explanation (str): Explanation for choosing the graphic variable.\n",
        "\n",
        "    Returns:\n",
        "        dict: Parsed JSON output containing visualization recommendations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the Prompt Template\n",
        "    geospatial_analysis_prompt = PromptTemplate(\n",
        "        input_variables=[\n",
        "            \"user_request\", \"insight_need_type\", \"chosen_visualization_type\", \"df_info\", \"df_head\", \"df_summary\",\n",
        "            \"chosen_graphic_symbol\", \"graphic_symbol_explanation\",\n",
        "            \"chosen_graphic_variable\", \"graphic_variable_explanation\"\n",
        "        ],\n",
        "        template=\"\"\"\n",
        "        You are a highly skilled data visualization expert specializing in geospatial analysis.\n",
        "        Your goal is to analyze the provided dataset and generate the best possible **geospatial visualizations**.\n",
        "\n",
        "        ---\n",
        "        ### **Geospatial Studies Context**\n",
        "        Geospatial Studies – \"Where\"\n",
        "        Geospatial analysis (or geostatistical analysis) helps answer \"Where\" questions by using statistical methods and spatial data visualization techniques. It focuses on understanding the location of objects, relationships between places, movement patterns, and spatial clustering. This is useful in fields such as geography, urban planning, epidemiology, and logistics.\n",
        "\n",
        "        Data Preprocessing\n",
        "        Before visualization, geospatial data needs geocoding (assigning latitude/longitude) and georeferencing (linking data to a map coordinate system).\n",
        "\n",
        "        Geocoding\n",
        "        - Converts addresses, postal codes, or geographic coordinates into mappable locations.\n",
        "        - Uses geographic gazetteers or GPS satellite triangulation to pinpoint positions.\n",
        "\n",
        "        Distance Calculation\n",
        "        - Measures shortest paths between locations using Great Circle Distance (Earth’s curvature).\n",
        "        - Used in routing, logistics, and disaster response planning.\n",
        "\n",
        "        Diffusion Matrices\n",
        "        - Used to model movement of tangible objects (e.g., vehicles, goods, money) and intangible elements (e.g., ideas, rumors, reputation).\n",
        "\n",
        "        Clustering\n",
        "        - Identifies patterns in geospatial data.\n",
        "        - Common methods: K-means clustering, density-based clustering, or predefined geographic classifications (e.g., census tracts).\n",
        "\n",
        "        Visual Generalization\n",
        "        - Large-scale maps simplify features (e.g., aggregation of cities into regions).\n",
        "        - Important for preserving map readability while retaining spatial accuracy.\n",
        "\n",
        "        Geospatial Visualization Types\n",
        "\n",
        "        1. Discrete Space Mapping\n",
        "        - Represents distinct data points with specific locations.\n",
        "        - Examples: Dot Density Maps (e.g., crime reports, disease cases).\n",
        "\n",
        "        2. Continuous Space Mapping\n",
        "        - Uses gradients to visualize non-discrete variables over an area.\n",
        "        - Examples:\n",
        "          - Elevation Maps (altitude visualization).\n",
        "          - Isarithmic Maps (temperature, pollution, population density).\n",
        "\n",
        "        3. Thematic Maps for Categorization\n",
        "        - Choropleth Maps: Color-coded regions based on data values (e.g., population density).\n",
        "        - Proportional Symbol Maps: Symbol sizes represent magnitudes (e.g., city population).\n",
        "\n",
        "        4. Cartograms (Distorted Maps)\n",
        "        - Adjusts area sizes based on data values.\n",
        "        - Example: A cartogram of world population enlarges countries like India and China.\n",
        "\n",
        "        5. Temporal-Spatial Representations\n",
        "        - Space-Time Cube Maps: Uses 3D coordinates (X, Y for space, Z for time).\n",
        "        - Flow Maps: Show directional movements (e.g., migration, trade routes).\n",
        "        - Strip Maps: Emphasize road networks and travel routes.\n",
        "\n",
        "        6. Specialized Maps for Movement Analysis\n",
        "        - Vector Fields: Depicts wind currents, ocean tides, or pressure systems.\n",
        "        - Isochrone Maps: Show travel time zones (e.g., 30-minute commute areas).\n",
        "        - Subway/Route Maps: Optimize transit systems with clean visual layouts.\n",
        "\n",
        "        Applications of Geospatial Analysis\n",
        "        1. Urban Planning – Identifying traffic congestion, housing patterns.\n",
        "        2. Disaster Response – Mapping flood-prone zones, evacuation routes.\n",
        "        3. Epidemiology – Tracking disease outbreaks.\n",
        "        4. Business Intelligence – Customer density heatmaps for market targeting.\n",
        "        5. Climate Science – Visualizing temperature shifts, deforestation.\n",
        "\n",
        "       ---\n",
        "        ### **User Request**\n",
        "        The user wants to visualize: \"{user_request}\"\n",
        "\n",
        "        ### **Insight Need Type**\n",
        "        - {insight_need_type}\n",
        "\n",
        "        ### **Chosen Visualization Type**\n",
        "        The visualization will use:\n",
        "        {chosen_visualization_type}\n",
        "\n",
        "        ---\n",
        "        ### **Dataset Overview**\n",
        "        **DataFrame Info:**\n",
        "        {df_info}\n",
        "\n",
        "        **DataFrame Sample Rows:**\n",
        "        {df_head}\n",
        "\n",
        "        **DataFrame Summary Statistics:**\n",
        "        {df_summary}\n",
        "\n",
        "        ---\n",
        "        ### **Chosen Graphic Symbol**\n",
        "        **Symbol:** {chosen_graphic_symbol}\n",
        "        **Reasoning:** {graphic_symbol_explanation}\n",
        "\n",
        "        ### **Chosen Graphic Variable**\n",
        "        **Variable:** {chosen_graphic_variable}\n",
        "        **Reasoning:** {graphic_variable_explanation}\n",
        "\n",
        "        ---\n",
        "        ### **Your Tasks**\n",
        "        - **Step 1:** Based on the dataset structure, insight need and chosen_visualization_type, select the **best** geospatial visualizations.\n",
        "        - **Step 2:** For the chosen visualization, list available **software tools** that can generate it (QGIS, Power BI, Tableau, Python, etc.).\n",
        "        - **Step 3:** Choose the most **suitable Python libraries** (preferably interactive ones like GeoPandas, Plotly, etc.). **DO NOT USE FOLIO** .\n",
        "        - **Step 4:** **DO NOT INCLUDE ANY PIP INSTALLS** in the code.\n",
        "        - **Step 5:** Identify any **data pre-processing** steps necessary before visualization.\n",
        "        - **Step 6:** Write **Python code** that:\n",
        "            - **Uses the existing dataset (df_filtered) in memory** without generating a new sample dataset.\n",
        "            - **DO NOT create a new DataFrame**. Use `df_filtered` directly.\n",
        "            - **Processes the dataset** correctly for the selected geospatial visualization.\n",
        "            - **Generates a high-quality map-based visualization** with:\n",
        "              - Proper **titles, data labels, legends, and color gradients** for readability.\n",
        "              - Interactive features if applicable.\n",
        "            - The script **must be fully executable** without requiring users to modify any placeholders.\n",
        "            - **Dynamically fetch** required datasets such as **GeoJSON files** or external geographic datasets.\n",
        "            - **DO NOT use placeholders** like `geojson_data = {{\"type\": \"FeatureCollection\", \"features\": []}}`.\n",
        "            - Include **error handling** for missing data or API failures.\n",
        "            - Ensure the **selected libraries match the visualization type**:\n",
        "              - If using **Plotly**, ensure the JSON structure matches its requirements and **ensure the map is correctly rendered by handling missing or mismatched location data appropriately to prevent empty maps.**\n",
        "            - **The code must execute successfully without any user modifications.**\n",
        "            - **Outputs the visualization.**\n",
        "        ---\n",
        "         ---\n",
        "        ### **Execution Environment: Google Colab**\n",
        "        This code will be executed in a **Google Colab environment**. Ensure that:\n",
        "        - **All visualizations are adapted for Colab compatibility**.\n",
        "        - **Ensure Google Colab compatibility** by adapting visualization display:\n",
        "        - **For Pyvis**, generate an HTML file and display using:\n",
        "          ```python\n",
        "          net.show(\"network_graph.html\")\n",
        "          from IPython.core.display import display, HTML\n",
        "          display(HTML(\"network_graph.html\"))\n",
        "\n",
        "        - **For Plotly,\n",
        "          - **ensure the map is correctly rendered by handling missing or mismatched location data appropriately to prevent empty maps.**\n",
        "\n",
        "        ---\n",
        "        ### **Expected JSON Output Format**\n",
        "        ```json\n",
        "        {{\n",
        "            \"visualization_1\": {{\n",
        "                \"name\": \"Geospatial Visualization Type 1\",\n",
        "                \"recommended_software\": [\"Software 1\", \"Software 2\", \"Software 3\"],\n",
        "                \"selected_libraries\": [\"Library 1\", \"Library 2\"],\n",
        "                \"preprocessing_steps\": \"Necessary preprocessing before visualization.\",\n",
        "                \"generated_code\": \"Python code for Geospatial Visualization 1 using df_filtered.\"\n",
        "            }}\n",
        "        }}\n",
        "        ```\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Create the Chain\n",
        "    geospatial_analysis_chain = LLMChain(\n",
        "        llm=llm,\n",
        "        prompt=geospatial_analysis_prompt,\n",
        "        output_key=\"geospatial_analysis_output\"\n",
        "    )\n",
        "\n",
        "    # Execute the Chain\n",
        "    geospatial_analysis_result = geospatial_analysis_chain.invoke({\n",
        "        \"user_request\": user_request,\n",
        "        \"insight_need_type\": insight_need_type,\n",
        "        \"chosen_visualization_type\": chosen_visualization_type,\n",
        "        \"df_info\": df_filtered.dtypes.to_string(),\n",
        "        \"df_head\": str(df_filtered.head()),\n",
        "        \"df_summary\": str(df_filtered.describe(include='all')),\n",
        "        \"chosen_graphic_symbol\": chosen_graphic_symbol,\n",
        "        \"graphic_symbol_explanation\": graphic_symbol_explanation,\n",
        "        \"chosen_graphic_variable\": chosen_graphic_variable,\n",
        "        \"graphic_variable_explanation\": graphic_variable_explanation\n",
        "    })\n",
        "\n",
        "    # Extract and Clean Output\n",
        "    geospatial_analysis_output = geospatial_analysis_result[\"geospatial_analysis_output\"]\n",
        "    cleaned_output = re.sub(r\"```json\", \"\", geospatial_analysis_output)\n",
        "    cleaned_output = re.sub(r\"```\", \"\", cleaned_output)\n",
        "\n",
        "    # Convert cleaned JSON string into a dictionary\n",
        "    try:\n",
        "        parsed_json = json.loads(cleaned_output)\n",
        "        for key, visualization in parsed_json.items():\n",
        "            markdown_output = f\"\"\"\n",
        "        **Visualization Name**\n",
        "        **{visualization.get('name', 'N/A')}**\n",
        "\n",
        "        **Recommended Software**\n",
        "        - {', '.join(visualization.get('recommended_software', []))}\n",
        "\n",
        "        **Selected Libraries**\n",
        "        - {', '.join(visualization.get('selected_libraries', []))}\n",
        "\n",
        "        **Preprocessing Steps**\n",
        "        {visualization.get('preprocessing_steps', 'N/A')}\n",
        "        \"\"\"\n",
        "        # Display dynamically formatted Markdown\n",
        "            display(Markdown(markdown_output))\n",
        "        return parsed_json\n",
        "    except json.JSONDecodeError:\n",
        "        raise ValueError(\"Invalid JSON response from LLM. Please check the output formatting.\")"
      ],
      "metadata": {
        "id": "5CCYQzPMrH8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### topical"
      ],
      "metadata": {
        "id": "uwWZvJzhrdgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_topical_analysis_chain(\n",
        "    llm,\n",
        "    user_request,\n",
        "    insight_need_type,\n",
        "    chosen_visualization_type,\n",
        "    df_filtered,\n",
        "    chosen_graphic_symbol,\n",
        "    graphic_symbol_explanation,\n",
        "    chosen_graphic_variable,\n",
        "    graphic_variable_explanation\n",
        "):\n",
        "    \"\"\"\n",
        "    Executes the topical analysis chain and returns the parsed JSON output.\n",
        "\n",
        "    Parameters:\n",
        "        llm: LangChain LLM instance\n",
        "        user_request (str): The user’s request for textual or topical visualization.\n",
        "        insight_need_type (str): The insight requirement output from Chain 1.\n",
        "        chosen_visualization_type (str): The visualization type determined from previous chains.\n",
        "        df_filtered (pd.DataFrame): The processed dataset.\n",
        "        chosen_graphic_symbol (str): Selected graphic symbol for visualization.\n",
        "        graphic_symbol_explanation (str): Explanation for choosing the graphic symbol.\n",
        "        chosen_graphic_variable (str): Selected graphic variable for visualization.\n",
        "        graphic_variable_explanation (str): Explanation for choosing the graphic variable.\n",
        "\n",
        "    Returns:\n",
        "        dict: Parsed JSON output containing visualization recommendations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the Prompt Template\n",
        "    topical_analysis_prompt = PromptTemplate(\n",
        "        input_variables=[\n",
        "            \"user_request\", \"insight_need_type\", \"chosen_visualization_type\",\n",
        "            \"df_info\", \"df_head\", \"df_summary\",\n",
        "            \"chosen_graphic_symbol\", \"graphic_symbol_explanation\",\n",
        "            \"chosen_graphic_variable\", \"graphic_variable_explanation\"\n",
        "        ],\n",
        "        template=\"\"\"\n",
        "        You are a highly skilled data visualization expert specializing in **textual and topical analysis**.\n",
        "        Your goal is to analyze the provided dataset and generate the best possible **topical visualizations**.\n",
        "\n",
        "        ---\n",
        "        ### **Topical Studies Context**\n",
        "        ### **Topical Studies—\"What\"**\n",
        "        Topical analysis is commonly applied to answer **“what”** questions by analyzing large-scale text corpora (e.g., articles, patents, grants, job applications, emails). These studies identify **term frequency distributions**, **topic composition**, and **textual trends** over time.\n",
        "\n",
        "        ---\n",
        "        ### **Data Preprocessing**\n",
        "        To extract meaningful insights from text, several preprocessing techniques are applied:\n",
        "\n",
        "        - **Fielding:** Identifies key sections of text (e.g., title, author name, address, abstract).\n",
        "        - **Text Selection:** Filters relevant text portions for analysis.\n",
        "        - **Stemming & Stopword Removal:** Reduces words to their base form and removes common words like \"the\" or \"of.\"\n",
        "        - **Tokenization:** Splits text into individual words, phrases, or **n-grams** (sequences of words).\n",
        "        - **Normalization:** Standardizes text by converting words into a consistent format for comparison.\n",
        "        - **Descriptive Term Identification:** Uses **TF-IDF (Term Frequency-Inverse Document Frequency)** to rank words based on their importance.\n",
        "        - **Tagging:** Assigns grammatical labels (e.g., noun, verb) to words for linguistic analysis.\n",
        "\n",
        "        ---\n",
        "        ### **Distributions in Text Analysis**\n",
        "        - **Term Frequency & Distributions:** Measures how often a word appears to determine importance.\n",
        "        - **Temporal Dynamics:** Tracks changes in word frequency over time (e.g., tracking the rise of certain terms in historical text).\n",
        "\n",
        "        ---\n",
        "        ## **Topical Visualization Types**\n",
        "        To visually represent textual data, various visualization techniques are used:\n",
        "\n",
        "        ### **Composition and Frequency**\n",
        "        - **Lists:** Represents term frequencies in ranked order.\n",
        "        - **Tag Clouds:** Words are displayed in various sizes based on their frequency.\n",
        "\n",
        "        ### **Graphs and Structure**\n",
        "        - **Topic Graphs:** Visualizes relationships between words or entities.\n",
        "        - **Circular Graphs:** Organizes words in a structured format for hierarchical representation.\n",
        "\n",
        "        ### **Crossmaps**\n",
        "        - **Scholarly Crossmaps:** Depict connections between authors, papers, and research topics.\n",
        "\n",
        "        ### **Trends and Evolution**\n",
        "        - **History Flow:** Displays **revision history** of documents using stacked lines.\n",
        "        - **Alluvial Graphs:** Tracks how topics **merge and split over time**.\n",
        "        - **Stream Graphs:** Represents thematic flows in text (metaphor of a river showing text evolution).\n",
        "        - **Arc Graphs:** Uses links to represent text relationships.\n",
        "\n",
        "        ### **Network Visualizations**\n",
        "        - **Text Networks:** Displays **associations and dependencies** between text entities (words, topics, references).\n",
        "\n",
        "        ---\n",
        "        ### **Execution Environment: Google Colab**\n",
        "        This code will be executed in a **Google Colab environment**. Ensure that:\n",
        "        - **All visualizations are adapted for Colab compatibility**.\n",
        "        - **Ensure Google Colab compatibility** by adapting visualization display:\n",
        "        - **For Pyvis**, generate an HTML file and display using:\n",
        "          ```python\n",
        "          net.show(\"network_graph.html\")\n",
        "          from IPython.core.display import display, HTML\n",
        "          display(HTML(\"network_graph.html\"))\n",
        "          ```\n",
        "\n",
        "        ---\n",
        "        ### **User Request**\n",
        "        The user wants to visualize: \"{user_request}\"\n",
        "\n",
        "        ### **Insight Need Type**\n",
        "        - {insight_need_type}\n",
        "\n",
        "        ### **Chosen Visualization Type**\n",
        "        The visualization will use:\n",
        "        {chosen_visualization_type}\n",
        "\n",
        "        ---\n",
        "        ### **Dataset Overview**\n",
        "        **DataFrame Info:**\n",
        "        {df_info}\n",
        "\n",
        "        **DataFrame Sample Rows:**\n",
        "        {df_head}\n",
        "\n",
        "        **DataFrame Summary Statistics:**\n",
        "        {df_summary}\n",
        "\n",
        "        ---\n",
        "        ### **Chosen Graphic Symbol**\n",
        "        **Symbol:** {chosen_graphic_symbol}\n",
        "        **Reasoning:** {graphic_symbol_explanation}\n",
        "\n",
        "        ### **Chosen Graphic Variable**\n",
        "        **Variable:** {chosen_graphic_variable}\n",
        "        **Reasoning:** {graphic_variable_explanation}\n",
        "\n",
        "        ---\n",
        "        ### **Your Tasks**\n",
        "        - **Step 1:** Based on the dataset structure, insight need, and chosen visualization type, select the **best** topical visualizations.\n",
        "        - **Step 2:** For the chosen visualization, list available **software tools** that can generate it (Power BI, Tableau, Python, etc.).\n",
        "        - **Step 3:** Choose the most **suitable Python libraries** (preferably interactive ones like WordCloud, Spacy, NLTK, Gensim, Plotly, etc.).\n",
        "        - **Step 4:** Identify any **data pre-processing** steps necessary before visualization.\n",
        "        - **Step 5:** Write **Python code** that:\n",
        "            - **Uses the existing dataset (`df_filtered`) in memory** without generating a new sample dataset.\n",
        "            - **DO NOT create a new DataFrame**. Use `df_filtered` directly.\n",
        "            - **Processes the dataset** correctly for the selected topical visualization.\n",
        "            - **Generates a high-quality textual visualization** with:\n",
        "              - Proper **titles, labels, font sizes, and color schemes** for readability.\n",
        "              - Interactive features if applicable.\n",
        "            - **Outputs the visualization.**\n",
        "\n",
        "        ---\n",
        "        ### **Expected JSON Output Format**\n",
        "        ```json\n",
        "        {{\n",
        "            \"visualization_1\": {{\n",
        "                \"name\": \"Topical Visualization Type 1\",\n",
        "                \"recommended_software\": [\"Software 1\", \"Software 2\", \"Software 3\", \"Software 4\", \"Software 5\"],\n",
        "                \"selected_libraries\": [\"Library 1\", \"Library 2\"],\n",
        "                \"preprocessing_steps\": \"Necessary preprocessing before visualization.\",\n",
        "                \"generated_code\": \"Python code for Topical Visualization 1 using df_filtered.\"\n",
        "            }}\n",
        "        }}\n",
        "        ```\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Create the Chain\n",
        "    topical_analysis_chain = LLMChain(\n",
        "        llm=llm,\n",
        "        prompt=topical_analysis_prompt,\n",
        "        output_key=\"topical_analysis_output\"\n",
        "    )\n",
        "\n",
        "    # Execute the Chain\n",
        "    topical_analysis_result = topical_analysis_chain.invoke({\n",
        "        \"user_request\": user_request,\n",
        "        \"insight_need_type\": insight_need_type,\n",
        "        \"chosen_visualization_type\": chosen_visualization_type,\n",
        "        \"df_info\": df_filtered.dtypes.to_string(),\n",
        "        \"df_head\": str(df_filtered.head()),\n",
        "        \"df_summary\": str(df_filtered.describe(include='all')),\n",
        "        \"chosen_graphic_symbol\": chosen_graphic_symbol,\n",
        "        \"graphic_symbol_explanation\": graphic_symbol_explanation,\n",
        "        \"chosen_graphic_variable\": chosen_graphic_variable,\n",
        "        \"graphic_variable_explanation\": graphic_variable_explanation\n",
        "    })\n",
        "\n",
        "    # Extract and Clean Output\n",
        "    topical_analysis_output = topical_analysis_result[\"topical_analysis_output\"]\n",
        "    cleaned_output = re.sub(r\"```json\", \"\", topical_analysis_output)\n",
        "    cleaned_output = re.sub(r\"```\", \"\", cleaned_output)\n",
        "\n",
        "    # Convert cleaned JSON string into a dictionary\n",
        "    try:\n",
        "        parsed_json = json.loads(cleaned_output)\n",
        "        for key, visualization in parsed_json.items():\n",
        "            markdown_output = f\"\"\"\n",
        "        **Visualization Name**\n",
        "        **{visualization.get('name', 'N/A')}**\n",
        "\n",
        "        **Recommended Software**\n",
        "        - {', '.join(visualization.get('recommended_software', []))}\n",
        "\n",
        "        **Selected Libraries**\n",
        "        - {', '.join(visualization.get('selected_libraries', []))}\n",
        "\n",
        "        **Preprocessing Steps**\n",
        "        {visualization.get('preprocessing_steps', 'N/A')}\n",
        "        \"\"\"\n",
        "        # Display dynamically formatted Markdown\n",
        "            display(Markdown(markdown_output))\n",
        "        return parsed_json\n",
        "    except json.JSONDecodeError:\n",
        "        raise ValueError(\"Invalid JSON response from LLM. Please check the output formatting.\")"
      ],
      "metadata": {
        "id": "SziU0mPnrX__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### network analysis"
      ],
      "metadata": {
        "id": "LuTL73P-rgTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_network_analysis_chain(\n",
        "    llm,\n",
        "    user_request,\n",
        "    insight_need_type,\n",
        "    chosen_visualization_type,\n",
        "    df_filtered,\n",
        "    chosen_graphic_symbol,\n",
        "    graphic_symbol_explanation,\n",
        "    chosen_graphic_variable,\n",
        "    graphic_variable_explanation\n",
        "):\n",
        "    \"\"\"\n",
        "    Executes the network analysis chain and returns the parsed JSON output.\n",
        "\n",
        "    Parameters:\n",
        "        llm: LangChain LLM instance\n",
        "        user_request (str): The user’s request for network visualization.\n",
        "        insight_need_type (str): The insight requirement output from previous chains.\n",
        "        chosen_visualization_type (str): The visualization type determined from previous chains.\n",
        "        df_filtered (pd.DataFrame): The processed dataset.\n",
        "        chosen_graphic_symbol (str): Selected graphic symbol for visualization.\n",
        "        graphic_symbol_explanation (str): Explanation for choosing the graphic symbol.\n",
        "        chosen_graphic_variable (str): Selected graphic variable for visualization.\n",
        "        graphic_variable_explanation (str): Explanation for choosing the graphic variable.\n",
        "\n",
        "    Returns:\n",
        "        dict: Parsed JSON output containing visualization recommendations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the Prompt Template\n",
        "    network_analysis_prompt = PromptTemplate(\n",
        "        input_variables=[\n",
        "            \"user_request\", \"insight_need_type\", \"chosen_visualization_type\",\n",
        "            \"df_info\", \"df_head\", \"df_summary\",\n",
        "            \"chosen_graphic_symbol\", \"graphic_symbol_explanation\",\n",
        "            \"chosen_graphic_variable\", \"graphic_variable_explanation\"\n",
        "        ],\n",
        "        template=\"\"\"\n",
        "        You are a highly skilled data visualization expert specializing in **network analysis**.\n",
        "        Your goal is to analyze the provided dataset and generate the best possible **network visualizations**.\n",
        "\n",
        "        ---\n",
        "        ### **Network Studies Context**\n",
        "        **Network Studies – \"With Whom\"**\n",
        "        Network analysis helps answer **\"Who interacts with whom?\"** by examining relationships between entities (nodes) and their connections (edges). It is widely used in:\n",
        "        - **Social Network Analysis (SNA)**\n",
        "        - **Citation & Collaboration Networks**\n",
        "        - **Supply Chain & Logistics**\n",
        "        - **Telecommunication Networks**\n",
        "        - **Traffic & Mobility Networks**\n",
        "\n",
        "        **Key Network Concepts**\n",
        "        - **Nodes**: Entities (e.g., people, companies, cities, publications).\n",
        "        - **Edges**: Relationships between nodes (e.g., friendships, citations, transactions).\n",
        "        - **Edge Weight**: Strength of connection (e.g., frequency of interactions).\n",
        "        - **Directed vs. Undirected Graphs**: Relationships may have directionality (e.g., follower/following relationships).\n",
        "        - **Centrality Measures**: Used to determine node importance.\n",
        "          - **Degree Centrality**: Number of direct connections.\n",
        "          - **Betweenness Centrality**: Measures node influence in connecting clusters.\n",
        "          - **Closeness Centrality**: Measures how quickly information spreads.\n",
        "\n",
        "        **Network Visualization Types**\n",
        "        1. **Node-Link Graphs**\n",
        "           - **Simple Network Graphs** – Shows basic node-to-node relationships.\n",
        "           - **Force-Directed Graphs** – Uses force simulation to improve node positioning.\n",
        "           - **Radial Network Graphs** – Displays hierarchical relationships (e.g., organizational charts).\n",
        "\n",
        "        2. **Hierarchical & Tree Structures**\n",
        "           - **Dendrograms** – Represents hierarchical clustering.\n",
        "           - **Tree Layouts** – Displays organizational or decision-making structures.\n",
        "\n",
        "        3. **Cluster & Community Detection**\n",
        "           - **Modular Networks** – Shows tightly connected subgroups.\n",
        "           - **Graph Partitioning** – Identifies distinct groups based on connectivity.\n",
        "\n",
        "        4. **Flow & Process Networks**\n",
        "           - **Sankey Diagrams** – Represents flow of resources, transactions, or influence.\n",
        "           - **Bipartite Graphs** – Shows relationships between two distinct node groups (e.g., buyers & sellers).\n",
        "\n",
        "        5. **Geospatial Network Analysis**\n",
        "           - **Route Maps** – Optimizes travel paths.\n",
        "           - **Supply Chain Networks** – Maps logistics & distribution.\n",
        "\n",
        "        ---\n",
        "        ### **User Request**\n",
        "        The user wants to visualize: \"{user_request}\"\n",
        "\n",
        "        ### **Insight Need Type**\n",
        "        - {insight_need_type}\n",
        "\n",
        "        ### **Chosen Visualization Type**\n",
        "        The visualization will use:\n",
        "        {chosen_visualization_type}\n",
        "\n",
        "        ---\n",
        "        ### **Dataset Overview**\n",
        "        **DataFrame Info:**\n",
        "        {df_info}\n",
        "\n",
        "        **DataFrame Sample Rows:**\n",
        "        {df_head}\n",
        "\n",
        "        **DataFrame Summary Statistics:**\n",
        "        {df_summary}\n",
        "\n",
        "        ---\n",
        "        ### **Chosen Graphic Symbol**\n",
        "        **Symbol:** {chosen_graphic_symbol}\n",
        "        **Reasoning:** {graphic_symbol_explanation}\n",
        "\n",
        "        ### **Chosen Graphic Variable**\n",
        "        **Variable:** {chosen_graphic_variable}\n",
        "        **Reasoning:** {graphic_variable_explanation}\n",
        "\n",
        "        ---\n",
        "        ### **Your Tasks**\n",
        "        - **Step 1:** Based on the dataset structure, insight need, and chosen visualization type, select the **best** network visualizations.\n",
        "        - **Step 2:** For the chosen visualization, list available **software tools** that can generate it (Gephi, Power BI, Cytoscape, Python, etc.).\n",
        "        - **Step 3:** Choose the most **suitable Python libraries** (preferably interactive ones like NetworkX, Plotly etc.).\n",
        "        - **Step 4:** Identify any **data pre-processing** steps necessary before visualization.\n",
        "        - **Step 5:** Write **Python code** that:\n",
        "            - **Uses the existing dataset (`df_filtered`) in memory** without generating a new sample dataset.\n",
        "            - **DO NOT create a new DataFrame**. Use `df_filtered` directly.\n",
        "            - **Processes the dataset** correctly for the selected network visualization.\n",
        "            - **Generates a high-quality network visualization** with:\n",
        "              - Proper **titles, labels, node colors, and edge weights** for readability.\n",
        "              - Interactive features if applicable.\n",
        "            - **Properly encodes the visualizations with graphic symbols and variables**.\n",
        "            - **Outputs the visualization.**\n",
        "\n",
        "        ---\n",
        "        ### **Expected JSON Output Format**\n",
        "        ```json\n",
        "        {{\n",
        "            \"visualization_1\": {{\n",
        "                \"name\": \"Network Visualization Type 1\",\n",
        "                \"recommended_software\": [\"Software 1\", \"Software 2\", \"Software 3\"],\n",
        "                \"selected_libraries\": [\"Library 1\", \"Library 2\"],\n",
        "                \"preprocessing_steps\": \"Necessary preprocessing before visualization.\",\n",
        "                \"generated_code\": \"Python code for Network Visualization 1 using df_filtered.\"\n",
        "            }}\n",
        "        }}\n",
        "        ```\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Create the Chain\n",
        "    network_analysis_chain = LLMChain(\n",
        "        llm=llm,\n",
        "        prompt=network_analysis_prompt,\n",
        "        output_key=\"network_analysis_output\"\n",
        "    )\n",
        "\n",
        "    # Execute the Chain\n",
        "    network_analysis_result = network_analysis_chain.invoke({\n",
        "        \"user_request\": user_request,\n",
        "        \"insight_need_type\": insight_need_type,\n",
        "        \"chosen_visualization_type\": chosen_visualization_type,\n",
        "        \"df_info\": df_filtered.dtypes.to_string(),\n",
        "        \"df_head\": str(df_filtered.head()),\n",
        "        \"df_summary\": str(df_filtered.describe(include='all')),\n",
        "        \"chosen_graphic_symbol\": chosen_graphic_symbol,\n",
        "        \"graphic_symbol_explanation\": graphic_symbol_explanation,\n",
        "        \"chosen_graphic_variable\": chosen_graphic_variable,\n",
        "        \"graphic_variable_explanation\": graphic_variable_explanation\n",
        "    })\n",
        "\n",
        "    # Extract and Clean Output\n",
        "    network_analysis_output = network_analysis_result[\"network_analysis_output\"]\n",
        "    cleaned_output = re.sub(r\"```json\", \"\", network_analysis_output)\n",
        "    cleaned_output = re.sub(r\"```\", \"\", cleaned_output)\n",
        "\n",
        "    # Convert cleaned JSON string into a dictionary\n",
        "    try:\n",
        "        parsed_json = json.loads(cleaned_output)\n",
        "        for key, visualization in parsed_json.items():\n",
        "            markdown_output = f\"\"\"\n",
        "        **Visualization Name**\n",
        "        **{visualization.get('name', 'N/A')}**\n",
        "\n",
        "        **Recommended Software**\n",
        "        - {', '.join(visualization.get('recommended_software', []))}\n",
        "\n",
        "        **Selected Libraries**\n",
        "        - {', '.join(visualization.get('selected_libraries', []))}\n",
        "\n",
        "        **Preprocessing Steps**\n",
        "        {visualization.get('preprocessing_steps', 'N/A')}\n",
        "        \"\"\"\n",
        "        # Display dynamically formatted Markdown\n",
        "            display(Markdown(markdown_output))\n",
        "        return parsed_json\n",
        "    except json.JSONDecodeError:\n",
        "        raise ValueError(\"Invalid JSON response from LLM. Please check the output formatting.\")"
      ],
      "metadata": {
        "id": "5QkgeYxrrf7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mjm1J7j8tYmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Function"
      ],
      "metadata": {
        "id": "Z9X3EtXht424"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### enter your google api key"
      ],
      "metadata": {
        "id": "kwR4q451yWau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
      ],
      "metadata": {
        "id": "pdlfJF5GuCCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b4c4bb-cacd-446d-a153-b4d37c723dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google AI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### load the data file"
      ],
      "metadata": {
        "id": "BksI1amKydOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "df = pd.read_csv('/content/state_M2023_dl+careerpathways.csv')\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Display the DataFrame\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg1QVH8FuTna",
        "outputId": "febd1b55-efa6-4a03-c4d3-64664b236689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 31855 entries, 0 to 35808\n",
            "Data columns (total 11 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   state                 31855 non-null  object \n",
            " 1   state_abbreviation    31855 non-null  object \n",
            " 2   occupation_title      31855 non-null  object \n",
            " 3   code                  31855 non-null  object \n",
            " 4   total_employment      31855 non-null  float64\n",
            " 5   hourly_wage_mean      31855 non-null  float64\n",
            " 6   annual_salary_mean    31855 non-null  object \n",
            " 7   hourly_wage_median    31855 non-null  float64\n",
            " 8   annual_salary_median  31855 non-null  object \n",
            " 9   career_pathway        31855 non-null  object \n",
            " 10  career_cluster        31855 non-null  object \n",
            "dtypes: float64(3), object(8)\n",
            "memory usage: 2.9+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### instantiate the llm"
      ],
      "metadata": {
        "id": "c1Ayl3cTyg7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initalize the llm api object\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")"
      ],
      "metadata": {
        "id": "zeuw1ylXuadi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dvl llms"
      ],
      "metadata": {
        "id": "56LflVoBypBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_choroplath_prompt = \"\"\"Show a choropleth map of how the average salary is distributed across the United States.\n",
        "Only include the **48 contiguous states**, **FILTER** Alaska, Hawaii, and U.S. territories (e.g., Puerto Rico, Guam) from the map.\n",
        "Ensure that the map is **accurately projected** and does not distort the continental U.S.\n",
        "\"\"\"\n",
        "\n",
        "example_sankey_prompt = \"\"\"Show how employment is distributed across career cluster, career pathway\n",
        " and specific occupations through a sankey diagram. Limit job roles only to top-20 occupations with the highest average salary (AVERAGE annual_salary_mean).\n",
        "\n",
        "- Use a **gradient-based color scheme** where career clusters, career pathways, and occupations have **distinct but related color intensities** to indicate hierarchy.\n",
        "- Use **lighter shades for higher-level categories (career clusters)** and **progressively darker shades** for more specific categories (career pathways and occupations).\n",
        "- Ensure **high contrast between adjacent nodes** while maintaining a visually cohesive color theme.\n",
        "- Use **semi-transparent colors for links** to emphasize flow without overwhelming the visualization.\n",
        "- Avoid overly saturated colors that may obscure readability.\n",
        "- Use **tooltips or annotations** where necessary to highlight insights.\n",
        "\n",
        "Choose color codings that enhance clarity and interpretation while being visually appealing.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Br5lGjJ6A1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_request = \"\"\"Show how employment is distributed across career cluster, career pathway\n",
        " and specific occupations through a sankey diagram. Limit job roles only to top-20 occupations with the highest average salary (AVERAGE annual_salary_mean).\n",
        "\n",
        "- Use a **gradient-based color scheme** where career clusters, career pathways, and occupations have **distinct but related color intensities** to indicate hierarchy.\n",
        "- Use **lighter shades for higher-level categories (career clusters)** and **progressively darker shades** for more specific categories (career pathways and occupations).\n",
        "- Ensure **high contrast between adjacent nodes** while maintaining a visually cohesive color theme.\n",
        "- Use **semi-transparent colors for links** to emphasize flow without overwhelming the visualization.\n",
        "- Avoid overly saturated colors that may obscure readability.\n",
        "- Use **tooltips or annotations** where necessary to highlight insights.\n",
        "\n",
        "Choose color codings that enhance clarity and interpretation while being visually appealing.\n",
        "\"\"\"\n",
        "\n",
        "insight_json = identify_insight(user_request, llm)\n",
        "\n",
        "identified_data_scale = identify_data_scale(\n",
        "    df=df,\n",
        "    insight_need_type=insight_json[\"insight_need_type\"],\n",
        "    key_variables=insight_json[\"key_variables\"],\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "df_filtered, column_data_types = post_process_data(df, identified_data_scale)\n",
        "\n",
        "study_type_result = select_best_analysis_type(\n",
        "    user_request=user_request,\n",
        "    insight_json=insight_json,\n",
        "    df_filtered=df_filtered,\n",
        "    column_data_types=column_data_types,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "chosen_visualization_type = choose_best_visualization(user_request, insight_json, df_filtered, study_type_result['chosen_study_type'], llm)\n",
        "\n",
        "chosen_graphic_symbol = choose_best_graphic_symbol(user_request, insight_json,\n",
        "                        df_filtered, column_data_types, chosen_visualization_type, study_type_result['chosen_study_type'], llm)\n",
        "\n",
        "chosen_graphic_variable = choose_best_graphic_variable(\n",
        "    user_request=user_request,\n",
        "    insight_json=insight_json,\n",
        "    df_filtered=df_filtered,\n",
        "    column_data_types=column_data_types,\n",
        "    chosen_graphic_symbol=chosen_graphic_symbol,\n",
        "    chosen_visualization_type=chosen_visualization_type,\n",
        "    chosen_analysis_type=study_type_result['chosen_study_type'],\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "chosen_study_type = study_type_result['chosen_study_type']\n",
        "\n",
        "if chosen_study_type == \"Temporal\":\n",
        "    result = run_temporal_analysis_chain(\n",
        "    llm=llm,\n",
        "      user_request=user_request,\n",
        "      insight_need_type=insight_json[\"insight_need_type\"],\n",
        "      chosen_visualization_type=chosen_visualization_type,\n",
        "      df_filtered=df_filtered,\n",
        "      chosen_graphic_symbol=chosen_graphic_symbol['chosen_graphic_symbol'],\n",
        "      graphic_symbol_explanation=chosen_graphic_symbol['reasoning'],\n",
        "      chosen_graphic_variable=chosen_graphic_variable['chosen_graphic_variable'],\n",
        "      graphic_variable_explanation=chosen_graphic_variable['reasoning']\n",
        ")\n",
        "elif chosen_study_type == \"Geospatial\":\n",
        "  result = run_geospatial_analysis_chain(\n",
        "    llm=llm,\n",
        "      user_request=user_request,\n",
        "      insight_need_type=insight_json[\"insight_need_type\"],\n",
        "      chosen_visualization_type=chosen_visualization_type,\n",
        "      df_filtered=df_filtered,\n",
        "      chosen_graphic_symbol=chosen_graphic_symbol['chosen_graphic_symbol'],\n",
        "      graphic_symbol_explanation=chosen_graphic_symbol['reasoning'],\n",
        "      chosen_graphic_variable=chosen_graphic_variable['chosen_graphic_variable'],\n",
        "      graphic_variable_explanation=chosen_graphic_variable['reasoning']\n",
        ")\n",
        "elif chosen_study_type == \"Topical\":\n",
        "  result = run_topical_analysis_chain(\n",
        "    llm=llm,\n",
        "      user_request=user_request,\n",
        "      insight_need_type=insight_json[\"insight_need_type\"],\n",
        "      chosen_visualization_type=chosen_visualization_type,\n",
        "      df_filtered=df_filtered,\n",
        "      chosen_graphic_symbol=chosen_graphic_symbol['chosen_graphic_symbol'],\n",
        "      graphic_symbol_explanation=chosen_graphic_symbol['reasoning'],\n",
        "      chosen_graphic_variable=chosen_graphic_variable['chosen_graphic_variable'],\n",
        "      graphic_variable_explanation=chosen_graphic_variable['reasoning']\n",
        ")\n",
        "elif chosen_study_type == \"Network\":\n",
        "  result = run_network_analysis_chain(\n",
        "      llm=llm,\n",
        "      user_request=user_request,\n",
        "      insight_need_type=insight_json[\"insight_need_type\"],\n",
        "      chosen_visualization_type=chosen_visualization_type,\n",
        "      df_filtered=df_filtered,\n",
        "      chosen_graphic_symbol=chosen_graphic_symbol['chosen_graphic_symbol'],\n",
        "      graphic_symbol_explanation=chosen_graphic_symbol['reasoning'],\n",
        "      chosen_graphic_variable=chosen_graphic_variable['chosen_graphic_variable'],\n",
        "      graphic_variable_explanation=chosen_graphic_variable['reasoning']\n",
        "  )\n",
        "\n",
        "execute_visualization_from_json(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pOnTDiE-uo5x",
        "outputId": "a8f96b78-7d69-4818-8b07-dee54f293713"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### The user request is:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Show how employment is distributed across career cluster, career pathway\n and specific occupations through a sankey diagram. Limit job roles only to top-20 occupations with the highest average salary (AVERAGE annual_salary_mean).\n\n- Use a **gradient-based color scheme** where career clusters, career pathways, and occupations have **distinct but related color intensities** to indicate hierarchy.\n- Use **lighter shades for higher-level categories (career clusters)** and **progressively darker shades** for more specific categories (career pathways and occupations).\n- Ensure **high contrast between adjacent nodes** while maintaining a visually cohesive color theme.\n- Use **semi-transparent colors for links** to emphasize flow without overwhelming the visualization.\n- Avoid overly saturated colors that may obscure readability.\n- Use **tooltips or annotations** where necessary to highlight insights.\n\nChoose color codings that enhance clarity and interpretation while being visually appealing.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Identified Insight Needs: "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Insight Need Type is Composition → Hierarchical Structure"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Key Variables to use : ['career cluster', 'career pathway', 'occupation', 'AVERAGE annual_salary_mean']"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Reasoning : The user wants to visualize the hierarchical relationship between career clusters, career pathways, and specific occupations, showing how employment is distributed across these levels. The Sankey diagram is explicitly requested, which is a suitable visualization for representing flow and proportions within a hierarchical structure. The request to limit occupations to the top 20 based on average salary adds a ranking element, but the primary goal is to understand the composition and flow of employment across the career hierarchy."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Data Scale Types"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```json\n{\n  \"variable_mappings\": {\n    \"career cluster\": \"career_cluster\",\n    \"career pathway\": \"career_pathway\",\n    \"occupation\": \"occupation_title\",\n    \"AVERAGE annual_salary_mean\": \"annual_salary_mean\"\n  },\n  \"categorized_variables\": {\n    \"career_cluster\": {\n      \"chosen_scale\": \"Nominal\",\n      \"suggested_python_dtype\": \"category\",\n      \"conversion_needed\": false,\n      \"conversion_code\": null\n    },\n    \"career_pathway\": {\n      \"chosen_scale\": \"Nominal\",\n      \"suggested_python_dtype\": \"category\",\n      \"conversion_needed\": false,\n      \"conversion_code\": null\n    },\n    \"occupation_title\": {\n      \"chosen_scale\": \"Nominal\",\n      \"suggested_python_dtype\": \"str\",\n      \"conversion_needed\": false,\n      \"conversion_code\": null\n    },\n    \"annual_salary_mean\": {\n      \"chosen_scale\": \"Ratio\",\n      \"suggested_python_dtype\": \"float\",\n      \"conversion_needed\": false,\n      \"conversion_code\": null\n    }\n  }\n}\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Data Pre-Processing:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Data before processing: \n "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```\nstate                   category\nstate_abbreviation        object\noccupation_title          object\ncode                      object\ntotal_employment         float64\nhourly_wage_mean         float64\nannual_salary_mean       float64\nhourly_wage_median       float64\nannual_salary_median      object\ncareer_pathway            object\ncareer_cluster            object\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Data after processing: \n "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```\ncareer_cluster         object\ncareer_pathway         object\noccupation_title       object\nannual_salary_mean    float64```"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Chosen Analysis Type"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Network"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The user request focuses on visualizing the hierarchical relationships and flow between career clusters, career pathways, and specific occupations. This aligns perfectly with the purpose of a network study, which examines relationships and interactions between entities. The Sankey diagram, explicitly requested by the user, is a common visualization technique used in network analysis to represent flows and connections between different categories. The dataset contains categorical variables representing these entities and a numerical variable (annual_salary_mean) that can be used to filter and prioritize the occupations displayed in the network."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Chosen Visualization Type"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sankey Diagram"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The user explicitly requested a Sankey diagram to visualize the hierarchical flow of employment from career clusters to career pathways to specific occupations. This visualization is ideal for showing the distribution and relationships between these categorical variables, fulfilling the 'Composition -> Hierarchical Structure' insight need. The data structure, consisting of career_cluster, career_pathway, occupation_title (all categorical) and annual_salary_mean (numerical), is suitable for a Sankey diagram, especially after filtering for the top 20 occupations by salary. The user's specifications for color gradients, transparency, and annotations further support the choice of a Sankey diagram to effectively communicate the hierarchical relationships and flow of employment across different categories."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Chosen Graphic Symbol"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Lines"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Given the user's request for a Sankey diagram to visualize the flow of employment across career clusters, career pathways, and occupations, 'Lines' are the most appropriate graphic symbol. Sankey diagrams inherently use lines (or flows/edges) to connect nodes representing different categories and show the magnitude of flow between them. The width of the lines typically represents the quantity or proportion of the flow. In this case, the lines will represent the number of employees transitioning from one career cluster to a specific career pathway and then to a particular occupation. The user's specifications for semi-transparent colors for the links further emphasize the importance of lines in conveying the flow of information without overwhelming the visualization. The categorical nature of career_cluster, career_pathway, and occupation_title, combined with the need to show hierarchical relationships and flow (Network analysis type, Composition -> Hierarchical Structure insight need), makes lines the ideal choice."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Chosen Graphic Variable"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Color (Hue & Value) + Transparency"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Given the user's request for a Sankey diagram to visualize the hierarchical flow of employment across career clusters, career pathways, and occupations, and the use of 'Lines' as the graphic symbol, 'Color (Hue & Value)' and 'Transparency' are the most effective graphic variables. \n\nColor (Hue & Value): Hue will be used to differentiate the main categories (career clusters), providing distinct visual identities. Value (lightness/darkness) will then be used within each cluster to represent the hierarchy, with lighter shades for higher-level categories (career clusters) and progressively darker shades for more specific categories (career pathways and occupations). This aligns perfectly with the user's request for a gradient-based color scheme to indicate hierarchy. The categorical nature of career_cluster, career_pathway, and occupation_title makes hue an appropriate choice for initial differentiation, while the ordinal nature of the hierarchy within each cluster makes value suitable for representing the levels.\n\nTransparency: Transparency will be applied to the lines (flows/edges) connecting the nodes. This addresses the user's requirement to emphasize flow without overwhelming the visualization. By making the lines semi-transparent, we can reduce visual clutter and allow the nodes to remain prominent. This is particularly important in a Sankey diagram where many lines may overlap. The degree of transparency can also be adjusted to reflect the magnitude of the flow, with more transparent lines representing smaller flows and less transparent lines representing larger flows, although this is a secondary consideration.\n\nCombining Color and Transparency allows us to effectively encode both the categorical and hierarchical aspects of the data, as well as the magnitude of the flow between categories, while adhering to the user's specific design requirements for clarity and visual appeal."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n        **Visualization Name**\n        **Sankey Diagram of Employment Distribution**\n\n        **Recommended Software**\n        - Tableau, Power BI, Python (Plotly), R (networkD3)\n\n        **Selected Libraries**\n        - Plotly\n\n        **Preprocessing Steps**\n        1. Filter the DataFrame to include only the top 20 occupations with the highest average salary.\n2. Aggregate the data to count the number of employees in each career cluster, career pathway, and occupation combination.\n3. Create the necessary data structure (lists of labels, sources, targets, and values) for the Sankey diagram.\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 Running: Sankey Diagram of Employment Distribution Visualization\n",
            "🔻 Python Code:\n",
            " import pandas as pd\n",
            "import plotly.graph_objects as go\n",
            "\n",
            "# Assuming df_filtered is already in memory and contains the data\n",
            "# Filter for top 20 occupations by average salary\n",
            "top_20_occupations = df_filtered.groupby('occupation_title')['annual_salary_mean'].mean().nlargest(20).index\n",
            "df_top20 = df_filtered[df_filtered['occupation_title'].isin(top_20_occupations)].copy()\n",
            "\n",
            "# Aggregate data for Sankey diagram\n",
            "sankey_data = df_top20.groupby(['career_cluster', 'career_pathway', 'occupation_title']).size().reset_index(name='count')\n",
            "\n",
            "# Create labels for all unique categories\n",
            "labels = list(sankey_data['career_cluster'].unique()) + list(sankey_data['career_pathway'].unique()) + list(sankey_data['occupation_title'].unique())\n",
            "\n",
            "# Create mappings from category to index\n",
            "label_map = {label: i for i, label in enumerate(labels)}\n",
            "\n",
            "# Create source, target, and value lists\n",
            "sources = []\n",
            "targets = []\n",
            "values = []\n",
            "\n",
            "# Add links from career cluster to career pathway\n",
            "for _, row in sankey_data.iterrows():\n",
            "    sources.append(label_map[row['career_cluster']])\n",
            "    targets.append(label_map[row['career_pathway']])\n",
            "    values.append(row['count'])\n",
            "\n",
            "# Add links from career pathway to occupation\n",
            "pathway_offset = len(sankey_data['career_cluster'].unique())\n",
            "for _, row in sankey_data.iterrows():\n",
            "    sources.append(label_map[row['career_pathway']])\n",
            "    targets.append(label_map[row['occupation_title']])\n",
            "    values.append(row['count'])\n",
            "\n",
            "# Define color scheme (gradient from light to dark within each cluster)\n",
            "num_clusters = len(sankey_data['career_cluster'].unique())\n",
            "cluster_colors = ['#ADD8E6', '#90CAF9', '#64B5F6', '#42A5F5', '#2196F3', '#1E88E5', '#1976D2', '#1565C0', '#0D47A1', '#0039A6'] # Example: shades of blue\n",
            "\n",
            "# Assign colors to nodes based on hierarchy\n",
            "node_colors = []\n",
            "for label in labels:\n",
            "    if label in sankey_data['career_cluster'].unique():\n",
            "        node_colors.append(cluster_colors[0])  # Lightest shade for career clusters\n",
            "    elif label in sankey_data['career_pathway'].unique():\n",
            "        node_colors.append(cluster_colors[4])  # Medium shade for career pathways\n",
            "    else:\n",
            "        node_colors.append(cluster_colors[8])  # Darkest shade for occupations\n",
            "\n",
            "# Create Sankey diagram\n",
            "data = go.Sankey(\n",
            "    node=dict(\n",
            "        pad=15,\n",
            "        thickness=20,\n",
            "        line=dict(color=\"black\", width=0.5),\n",
            "        label=labels,\n",
            "        color=node_colors\n",
            "    ),\n",
            "    link=dict(\n",
            "        source=sources,\n",
            "        target=targets,\n",
            "        value=values,\n",
            "        color=['rgba(0,0,0,0.2)'] * len(sources)  # Semi-transparent links\n",
            "    )\n",
            ")\n",
            "\n",
            "layout = dict(title=\"Employment Distribution Across Career Clusters, Pathways, and Occupations (Top 20 Salaries)\", font=dict(size=12))\n",
            "\n",
            "fig = go.Figure(data=[data], layout=layout)\n",
            "fig.show()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1d1d2a20-54e3-4ea0-9b74-42e6e39f4f24\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1d1d2a20-54e3-4ea0-9b74-42e6e39f4f24\")) {                    Plotly.newPlot(                        \"1d1d2a20-54e3-4ea0-9b74-42e6e39f4f24\",                        [{\"link\":{\"color\":[\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\",\"rgba(0,0,0,0.2)\"],\"source\":[0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3],\"target\":[2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],\"value\":[45,26,34,26,38,49,44,28,43,43,33,11,13,30,5,52,32,44,34,38,45,26,34,26,38,49,44,28,43,43,33,11,13,30,5,52,32,44,34,38]},\"node\":{\"color\":[\"#ADD8E6\",\"#ADD8E6\",\"#2196F3\",\"#2196F3\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\",\"#0D47A1\"],\"label\":[\"General Management\",\"Therapeutic Services\",\"Business Management & Administration\",\"Health Science\",\"Chief Executives\",\"Anesthesiologists\",\"Cardiologists\",\"Dermatologists\",\"Emergency Medicine Physicians\",\"Family Medicine Physicians\",\"General Internal Medicine Physicians\",\"Neurologists\",\"Nurse Anesthetists\",\"Obstetricians and Gynecologists\",\"Ophthalmologists, Except Pediatric\",\"Oral and Maxillofacial Surgeons\",\"Orthodontists\",\"Orthopedic Surgeons, Except Pediatric\",\"Pediatric Surgeons\",\"Physicians, All Other\",\"Physicians, Pathologists\",\"Psychiatrists\",\"Radiologists\",\"Surgeons, All Other\"],\"line\":{\"color\":\"black\",\"width\":0.5},\"pad\":15,\"thickness\":20},\"type\":\"sankey\"}],                        {\"font\":{\"size\":12},\"title\":{\"text\":\"Employment Distribution Across Career Clusters, Pathways, and Occupations (Top 20 Salaries)\"},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1d1d2a20-54e3-4ea0-9b74-42e6e39f4f24');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully executed: Sankey Diagram of Employment Distribution\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cT-DMiPOlU_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}